# Load the course script
source("https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/main/courseUtil.R")

# Load the Myers' (2015) clean data set to revisit the test of equal variance
# (aka F-test)
Myers.clean = loadCourseCSV(2024, "5_ANOVA", "MyersClean.csv")

# Extract the logRT of the two experiment sessions to test if the two samples
# have an equal variance
s1.rt = Myers.clean[Myers.clean$Session == 1,]$logRT
s2.rt = Myers.clean[Myers.clean$Session == 2,]$logRT

# Check the report of the F-test generated by var.test(); check the Unit 5
# handout for detailed explanations.
var.test(s1.rt, s2.rt)

# F-test formula: s2x / s2y
var(s1.rt) / var(s2.rt)
# Get the p-value from an F distribution with two dfs derived from the
# numerator and denominator samples. The F value is higher than 1, which is the
# peak of an F distribution, so get an upper tail p-value first.
pf(q = 1.160577, df1 = 6750, df2 = 7200, lower.tail = F)
# Double the one-tailed p to get a two-tailed p, because the null hypothesis
# in an F-test is that the two variances are the same, which means that the 
# alternative hypothesis is that the numerator is higher/lower than the
# denominator.
pf(q = 1.160577, df1 = 6750, df2 = 7200, lower.tail = F) * 2

# Extend the F-test to a one-way independent-measures ANOVA
# Check the Unit 5 handout for the explanations of "one-way",
# "independent measures", and its null/alternative hypothesis.
# Check the Unit 5 handout for the detailed introduction to Saito & Lyster's
# (2012) study
sl.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterSim.csv")

# Use the y ~ x syntax (explain the dependent variable y with the independent
# variable/factor x) to build an ANOVA model and fit the model to the data
sl.sim.aov = aov(formula = rF3 ~ Group, data = sl.sim)
# Get the analysis summary
summary(sl.sim.aov)

# Validate the F-value: between-sample Mean Sq / within-sample Mean Sq (aka MSE)
5345500 / 107131

# Manually calculate SS, F, and p-value in one-way independent-measures ANOVA
# The global mean: The average value of the entire sample
mean.global = mean(sl.sim$rF3)

# Extract the three subsets based on Group
sl.sim.control = subset(sl.sim, Group == "Control")
sl.sim.ffi = subset(sl.sim, Group == "FFI")
sl.sim.fficf = subset(sl.sim, Group == "FFI+CF")

# Calculate the three sample means
mean.control = mean(sl.sim.control$rF3)
mean.ffi = mean(sl.sim.ffi$rF3)
mean.fficf = mean(sl.sim.fficf$rF3)

# Calculate the size of each sample
control.size = nrow(sl.sim.control)
ffi.size = nrow(sl.sim.ffi)
fficf.size = nrow(sl.sim.fficf)

# Calculate the between-sample sum of squares (SS-between): The squared differences
# between individual sample means and the global mean
ss.between = control.size * (mean.control - mean.global) ^ 2 +
  ffi.size * (mean.ffi - mean.global) ^ 2 +
  fficf.size * (mean.fficf - mean.global) ^ 2

# df-between is the number of levels in the only fixed factor (aka independent
# variable) minus 1
df.between = 3 - 1

# Between-sample mean squared difference, or MS. The interesting variance.
ss.between / df.between

# Calculate the within-sample sum of squares (SS-within): The squared differences
# between individual data points in each sample and their sample mean.
ss.control = sum((sl.sim.control$rF3 - mean.control) ^ 2)
ss.ffi = sum((sl.sim.ffi$rF3 - mean.ffi) ^ 2)
ss.fficf = sum((sl.sim.fficf$rF3 - mean.fficf) ^ 2)
ss.within = ss.control + ss.ffi + ss.fficf

# df-within is the sum of each sample size minus 1
df.within = control.size - 1 + ffi.size - 1 + fficf.size - 1

# Get within-sample mean squared difference: The boring variance.
ss.within / df.within

# Get the one-tailed p-value using pf() based on the F-value and the two dfs.
# We only need to calculate the UPPER TAIL p-value because the only thing we
# want to know is whether the interesting variance is significantly higher than
# the boring variance (i.e., F is significantly higher than 1).
pf(q = (ss.between / df.between) / (ss.within / df.within), df1 = df.between,
   df2 = df.within, lower.tail = F)

# Leave out the FFI+CF so the Group factor only has two levels (FFI vs. Control)
# to demonstrate the similarity between an one-way independent-measures ANOVA
# and an unpaired two-sample t-test ASSUMING AN EQUAL VARIANCE. See the Unit 5
# handout for detailed explanations.
sl.sim.sub = subset(sl.sim, Group != "FFI+CF")
# Run the ANOVA
sl.sim.sub.aov = aov(formula = rF3 ~ Group, data = sl.sim.sub)
# Show the stats
summary(sl.sim.sub.aov)

# Run the unpaired t-test assuming an equal variance; the same df and p-value
# can be found.
t.test(formula = rF3 ~ Group, data = sl.sim.sub, var.equal = T)

# In this comparison, the squared t-value is equal to the F-value.
1.3268 ^ 2

# For pairwise comparisons, Bonferroni correction, and the TukeyHSD test, see 
# Unit 5 handout for detailed explanations.

# In the full ANOVA model, we can make three pairwise comparisons, so the default
# alpha level .05 should be divided by 3, which is equal to .0167
bonferroni.a = .05 / 3

# Make pairwise comparisons between every two levels in Group to test between-level
# differences. Only the FFI-Control comparison does not have a p-value lower than
# 0.167
t.test(sl.sim.fficf$rF3, sl.sim.control$rF3, var.equal = T)
t.test(sl.sim.ffi$rF3, sl.sim.control$rF3, var.equal = T)
t.test(sl.sim.fficf$rF3, sl.sim.ffi$rF3, var.equal = T)

# Apply the TukeyHSD test to the entire ANOVA model for pairwise comparisons.
# Again, only the FFI-Control comparison does not have an adjusted p-value
# lower than .05
TukeyHSD(sl.sim.aov)

# One-way repeated-measures ANOVA
# See Unit 5 handout for the explanation of the within-subject design version
# of Saito and Lyster's simulated data
sl.rep.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterRepSim.csv")

# Calculate the by-subject rF3 average. WARNING: In real life, you SHOULD NOT
# do this because by-item variance also provides important information in
# statistical tests. I do this only for this demo. Please refer to Appendix B
# of the Unit 5 handout to see how to incorporate both by-subject and by-item
# analysis in ANOVA.
sl.rep.avg = aggregate(rF3 ~ Subject + Condition, FUN = mean, data = sl.rep.sim)

# Run the independent-measures ANOVA just for comparison; this is NOT the most
# appropriate ANOVA for a study with a within-subject design.
sl.rep.ind.aov = aov(formula = rF3 ~ Condition, data = sl.rep.avg)
summary(sl.rep.ind.aov)

# Run the MORE APPROPRIATE repeated-measures ANOVA
# First, convert Subject into a factor so the subject ID numbers are treated as 
# "unit labels" rather than numeric values.
sl.rep.avg$Subject = as.factor(sl.rep.avg$Subject)
# Build the ANOVA model; the Error() term means "Condition is a within-subject
# design factor, so partition out the between-subject variance across the three
# paired samples.
sl.aov.rep = aov(formula = rF3 ~ Condition + Error(Subject / Condition),
                 data = sl.rep.avg)

# Compare the two ANOVA outputs and see the similarities and the differences.
# See the Unit 5 handout for detailed explanations.
summary(sl.aov.rep)

# Calculate the stats in one-way repeated-measures ANOVA manually to validate
# the numbers seen in sl.aov.rep

# The global/grand mean of rF3
mean.rep.global = mean(sl.rep.avg$rF3)

# The total variance (Sum of Squares): The squared differences between
# every single data point and the global mean
ss.rep.total = sum((sl.rep.avg$rF3 - mean.rep.global) ^ 2)
ss.rep.total

# Get the three paired samples based on Condition
sl.rep.cl = subset(sl.rep.avg, Condition == "Control")
sl.rep.ffi = subset(sl.rep.avg, Condition == "FFI")
sl.rep.fficf = subset(sl.rep.avg, Condition == "FFI+CF")

# Get the size of each paired sample
sl.rep.cl.n = nrow(sl.rep.cl)
sl.rep.ffi.n = nrow(sl.rep.ffi)
sl.rep.fficf.n = nrow(sl.rep.fficf)

# Get the mean rF3 of each paired sample
sl.rep.cl.mean = mean(sl.rep.cl$rF3)
sl.rep.ffi.mean = mean(sl.rep.ffi$rF3)
sl.rep.fficf.mean = mean(sl.rep.fficf$rF3)

# Calculate the interesting variance (the between-sample SS); this part is the
# same as in a one-way independent-measures ANOVA
ss.rep.between = sl.rep.cl.n * (sl.rep.cl.mean - mean.rep.global) ^ 2 +
                  sl.rep.ffi.n * (sl.rep.ffi.mean - mean.rep.global) ^ 2 +
                  sl.rep.fficf.n * (sl.rep.fficf.mean - mean.rep.global) ^ 2
ss.rep.between

# Calculate the by-subject average rF3
sl.rep.subj.mean = aggregate(rF3 ~ Subject, FUN = mean, data = sl.rep.avg)
sl.rep.subj.mean

# Calculate the between-unit (between-subject) variance; the number of levels
# in the independent variable multiplies the sum of the squared differences
# between each by-subject mean and the global mean.
ss.between_unit = 3 * sum((sl.rep.subj.mean$rF3 - mean.rep.global) ^ 2)
ss.between_unit

# Subtracting ss-between-sample and ss-between-unit from ss-total to get
# ss-error (SSE).
ss.rep.total - ss.rep.between - ss.between_unit

# TukeyHSD() doesn't work for repeated-measures ANOVA models, so it is not
# possible to run Tukey's HSD test for post-hoc pairwise comparison.
TukeyHSD(sl.aov.rep)

# That's OK, we still have Bonferroni correction. There are three pairwise
# comparisons (CL vs. FFI, CL vs. FFI+CF, FFI vs. FFI+CF), so the 
# Bonferroni-corrected alpha is .05 / 3 = .016666666.....
.05 / 3

# Run PAIRED t-tests for the post-hoc pairwise comparisons
# CL vs. FFI; not significant
cl.ffi.t = t.test(sl.rep.cl$rF3, sl.rep.ffi$rF3, paired = T)
cl.ffi.t$p.value

# CL vs. FFI+CF; significant, just below the Bonferroni-corrected alpha
cl.fficf.t = t.test(sl.rep.cl$rF3, sl.rep.fficf$rF3, paired = T)
cl.fficf.t$p.value

# FFI vs. FFI+CF; significant
ffi.fficf.t = t.test(sl.rep.ffi$rF3, sl.rep.fficf$rF3, paired = T)
ffi.fficf.t$p.value

# Leave out the Control group, so the Condition now only has two levels.
# In this case, one-way repeated-measures ANOVA is just like a PAIRED t-test
sl.rep.aov.t = aov(rF3 ~ Condition + Error(Subject / Condition),
                   data = subset(sl.rep.avg, Condition != "Control"))
# Check the output, and the p-value is exactly the same as in the last
# pairwise comparison above.
summary(sl.rep.aov.t)

# If you square the t-value of the last pairwise comparison, you also get the 
# F-value of the above repeated-measures ANOVA.
ffi.fficf.t$statistic ^ 2

# Two-way independent-measures ANOVA
# Check the Unit 5 handout for (1) the explanations of interactions between
# two independent variables and (2) the introduction to Chen's (2020) study
# related to the sample data set.
chen.sample = loadCourseCSV(2024, "5_ANOVA", "Chen2020Sample.csv")

# Build a two-way independent-measures ANOVA model
chen.aov = aov(formula = Accept ~ Group * InitialTone, data = chen.sample)
# See also the Unit 5 handout for the detailed explanation of the output
summary(chen.aov)

# Group * InitialTone = Group + InitialTone + Group:InitialTone. Put differently,
# the model has two main effects and a two-way interaction.
chen.aov2 = aov(formula = Accept ~ Group + InitialTone + Group:InitialTone, 
                data = chen.sample)
# The output is exactly the same
summary(chen.aov2)

# This part is NOT included in the Unit 5 handout.
# First of all, InitialTone is in fact a within-subject design factor; all
# participants were tested with the same test items with either H or R on the
# first syllable. So, the most correct version of ANOVA for analyzing the data
# set is "mixed ANOVA" - one between-subject design factor (Group) and one
# within-subject design factor (InitialTone).

# First, convert the "participant" column into a factor so the participant ID
# numbers represent "unit labels".
chen.sample$participant = as.factor(chen.sample$participant)
# The logic is similar to one-way repeated-measures ANOVA; using Error() to
# specify the within-subject design variable to partition by-subject variance.
chen.aov3 = aov(formula = Accept ~ Group * InitialTone + 
                  Error(participant / InitialTone), data = chen.sample)
# The output is different from the one generated by the two-way 
# independent-measures ANOVA; Group no longer has a significant mean effect.
# It is the InitialTone and the interaction between Group and InitialTone that
# significantly contribute to the "interesting variance".
summary(chen.aov3)

xtabs(~ Group + InitialTone, data = chen.sample)

chen.sample.n = 1536

chen.sample.global.m = mean(chen.sample$Accept)

chen.sample.int.m = aggregate(Accept ~ Group + InitialTone, 
                              FUN = mean, data = chen.sample)

chen.sample.gr.m = aggregate(Accept ~ Group, FUN = mean, data = chen.sample)
chen.sample.ini.m = aggregate(Accept ~ InitialTone, FUN = mean, data = chen.sample)

# Group = *NonFinalH, InitialTone = H
chen.sample.n * (0.4602865 - chen.sample.global.m - 
                   (0.4947917 - chen.sample.global.m) - (0.5361328 - chen.sample.global.m)) ^ 2 +
# Group = *NonFinalR, InitialTone = H
  chen.sample.n * (0.6119792 - chen.sample.global.m - 
                     (0.5283203 - chen.sample.global.m) - (0.5361328 - chen.sample.global.m)) ^ 2 +
# Group = *NonFinalH, InitialTone = R
  chen.sample.n * (0.5292969 - chen.sample.global.m - 
                     (0.4947917 - chen.sample.global.m) - (0.4869792 - chen.sample.global.m)) ^ 2 +
# Group = *NonFinalR, InitialTone = R
  chen.sample.n * (0.4446615 - chen.sample.global.m - 
                     (0.5283203 - chen.sample.global.m) - (0.4869792 - chen.sample.global.m)) ^ 2

chen.aov.3 = aov(formula = Accept ~ InitialTone * Group, data = chen.sample)
summary(chen.aov.3)

chen.sample.unbal = chen.sample[-(1:50),]
xtabs(~ Group + InitialTone, chen.sample.unbal)

chen.aov.4 = aov(formula = Accept ~ InitialTone * Group, data = chen.sample.unbal)
summary(chen.aov.4)

chen.aov.5 = aov(formula = Accept ~ Group * InitialTone, data = chen.sample.unbal)
summary(chen.aov.5)

sl.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterSim.csv")

sl.sim.aov = aov(formula = rF3 ~ Group, data = sl.sim)
# Get the analysis summary
summary(sl.sim.aov)

10691001 / (10691001 + 68242430)

summary(chen.aov)

chen.sample.ss.total = 1.7 + 3.7 + 21.4 + 1508.3
1.7 / chen.sample.ss.total
3.7 / chen.sample.ss.total
21.4 / chen.sample.ss.total
