numProp = (sum(smallNum == TRUE) + sum(bigNum == TRUE)) / 10000
# This is still consider a normal distribution based on the proportion sampled,
# as it is still below 5%
numProp
#3
plot(density(randNums), main = "Theoretical Distribution")
abline(v = qnorm(p = 0.025, mean = 0.55, sd = 0.025), col = "green", lwd = 1.5, lty = 2)
abline(v = qnorm(p = 0.025, mean = 0.55, sd = 0.025, lower.tail = F), col = "green", lwd = 1.5, lty = 2)
# Part II
# Part II
library(languageR)
head(verbs)
#1
verbs.avg.lot = aggregate(x = LengthOfTheme ~ Verb, FUN = mean, data = verbs)
#2
verbs.ord = verbs.avg.lot[order(verbs.avg.lot$LengthOfTheme, decreasing = F), ]
rownames(verbs.ord) = 1:nrow(verbs.ord)
verbs.sub = subset(verbs.ord, rownames(verbs.ord) <= 20)
#3
barplot(height = verbs.avg.lot$LengthOfTheme, names.arg = verbs.avg.lot$Verb,
main = "Average Length Of Theme by Verb",
xlab = "", ylab = "LengthOfTheme", las = 2)
verbs.sub
rownames(verbs.ord)
head(verbs.ord)
1:nrow(verbs.ord)
verbs.sub
head(verbs.ord)
verbs.ord = verbs.avg.lot[order(verbs.avg.lot$LengthOfTheme, decreasing = F), ]
head(verbs.ord)
rownames(verbs.ord) = 1:nrow(verbs.ord)
head(verbs.ord)
rownames(verbs.ord) <= 20
# Part I
# Q1: Assuming a normal distribution of reaction times in a linguistic experiment
#     with a mean of 550 ms and an SD of 25 ms, calculate the two boundaries in
#     the distribution that help indicate a 2.5% chance in the upper and lower tails.
upperBoundary = 550 + (-1.96) * 25
lowerBoundary = 550 + 1.96 * 25
upperBoundary
lowerBoundary
# Q2: Randomly sample 10,000 values from the same normal distribution from (1)
#     and calculate the proportion of values that are higher or lower than the
#     two boundaries.
set.seed(10)
randNums = rnorm(n = 10000, mean = 550, sd = 25)
outliers = randNums < upperBoundary | randNums > lowerBoundary
outliersNum = sum(outliers)
Proportion = outliersNum / 10000
outliersNum
Proportion
# Q2: Randomly sample 10,000 values from the same normal distribution from (1)
#     and calculate the proportion of values that are higher or lower than the
#     two boundaries.
set.seed(10)
randNums = rnorm(n = 10000, mean = 550, sd = 25)
outliers = randNums < upperBoundary | randNums > lowerBoundary
outliersNum = sum(outliers)
Proportion = outliersNum / 10000
outliersNum
Proportion
# This proportion is slightly larger than 5% (i.e., 0.0501), so it doesn't perfectly
# align  with the theoretical expectation for a normal distribution. However,
# it still kind of consistent with the expected behavior of a normal distribution,
# where about 95% of the values fall between the lower and upper boundaries, and
# 5% fall outside (2.5% in each tail).
# Q3: Generate the density plot of the random sample in (2) with the vertical lines
#     that mark the two boundaries as well as a proper title.
plot(density(randNums), main = "Density Plot of randNums with Upper and Lower Boundaries")
abline(v = upperBoundary, col = "red", lwd = 1.5)
abline(v = lowerBoundary, col = "blue", lwd = 1.5)
# Part II
library(languageR)
head(verbs)
# Q1: Use aggregate() to generate a data frame that represents the average length
#     of the theme of each verb, and save this data frame separately as verbs.avg.lot.
verbs.avg.lot = aggregate(LengthOfTheme ~ Verb, FUN = mean, data = verbs)
head(verbs.avg.lot)
# Q2: Sort verbs.avg.lot based on the average length of the theme in increasing
#     order and save the sorted data frame as verbs.ord. Then, extract a data
#     subset from verbs.ord that includes only the Top-20 average theme lengths
#     as verbs.sub.
verbs.ord = verbs.avg.lot[order(verbs.avg.lot$LengthOfTheme), ]
head(verbs.ord)
verbs.sub = head(verbs.ord, 20)
verbs.sub
# Q3: With verbs.avg.lot, generate an informative bar plot using barplot().
# To make the plot more informative, we just need to include the verbs with an
# average Length of Theme that is above 0. Hence, I use -c() to delete the rows
# of the verbs with Length of Theme = 0.
LengthAbove0 = verbs.avg.lot[-c(8, 20, 21, 22, 44, 48, 49, 58),]
# We need to Set the y limit to 3 because the longest Length of Theme is 2.8.
barplot(height = LengthAbove0$LengthOfTheme,
names.arg = LengthAbove0$Verb, ylim = c(0, 3),
main = "Average Length of Theme for Each Verb (Length > 0)", xlab = " ",
ylab = "Length of Theme", las = 2)
# Q4: Create a 2 × 2 contingency table with the original data frame verbs, which
#     crosses the two factors “the animacy of themes” and “the animacy of receivers”
#     to show the number of each possible combination between factor levels.
table = xtabs(formula = ~ AnimacyOfTheme + AnimacyOfRec, data = verbs)
table
# Assignment IV
# Part I
# 1. Calculate the two boundaries for a normal distribution of reaction times
mean_rt <- 550  # Mean reaction time in milliseconds
sd_rt <- 25     # SD in milliseconds
# Calculate the lower and upper boundaries (2.5% each)
lower_bound <- qnorm(0.025, mean = mean_rt, sd = sd_rt)
upper_bound <- qnorm(0.975, mean = mean_rt, sd = sd_rt)
# Show the calculated boundaries
lower_bound  # Lower boundary
upper_bound  # Upper boundary
# 2. Randomly sample 10,000 values from the distribution and calculate the proportion outside the boundaries
# Set seed
set.seed(100)
# Randomly sample 10,000 values
sampled_values <- rnorm(10000, mean = mean_rt, sd = sd_rt)
# Check if values are outside the boundaries
outside_boundaries <- (sampled_values < lower_bound) | (sampled_values > upper_bound)
warnings()
# Calculate the proportion of values outside the boundaries
proportion_outside <- mean(outside_boundaries)
# Show the proportion
proportion_outside
# 3. Generate a density plot of the sampled values with vertical lines marking the boundaries
# Generate the density plot
plot(density(sampled_values), main = "Randomly Sampled Reaction Times",
xlab = "Reaction Time (ms)")
# Add vertical lines for the boundaries
abline(v = lower_bound, col = "red", lty = 2)
abline(v = upper_bound, col = "red", lty = 2)
# Part II:
# Load the languageR package
library(languageR)
head(verbs)  # Confirm that the data is loaded correctly
# 1. Calculate the average theme length for each verb
verbs.avg.lot <- aggregate(LengthOfTheme ~ Verb, data = verbs, FUN = mean)
# Show the resulting data frame
verbs.avg.lot
# 2. Sort the data frame by the average theme length and extract the top 20
verbs.ord <- verbs.avg.lot[order(verbs.avg.lot$LengthOfTheme), ]  # Sorted by average length
verbs.sub <- head(verbs.ord, 20)  # Extract the top 20
head(verbs.sub)
# Show the sorted data frame and the top 20 subset
verbs.ord
verbs.sub
# 3. Generate a bar plot to visualize the average theme length of each verb
barplot(verbs.ord$LengthOfTheme, names.arg = verbs.ord$Verb, las = 2,
main = "Average Length of Theme by Verb", ylab = "Average Length (Characters)")
# 4. Create a contingency table for animacy of themes and receivers
contingency_table <- table(verbs$AnimacyOfTheme, verbs$AnimacyOfRec)
# Show the contingency table
print("Rows represent the Animacy of Themes; Columns represent the Animacy of Receivers")
contingency_table
#1
sd.lower = 550 + qnorm(0.025) * 25
sd.upper = 550 + qnorm(0.975) * 25
sd.lower
sd.upper
#2
set.seed(100)
sample.v = rnorm(n = 10000, mean = 550, sd = 25)
outliers = sample.v < sd.lower | sample.v > sd.upper
number.v = sum(outliers)
proportion.v = number.v / 10000
proportion.v
plot(density(sample.v), main = " Reaction Times with Outlier Boundaries (density)",
xlab = "Reaction Times (ms)")
abline(v = sd.upper, col = "red", lwd = 2)
abline(v = sd.lower, col = "red", lwd = 2)
library(languageR)
head(verbs)
#1
verbs.avg.lot = aggregate(LengthOfTheme ~ Verb, data = verbs, FUN = mean)
verbs.avg.lot
#2
verbs.ord = verbs.avg.lot[order(verbs.avg.lot$LengthOfTheme), ]
verbs.sub = head(verbs.ord, 20)
verbs.sub
#3
barplot(height = verbs.avg.lot$LengthOfTheme,
names.arg = verbs.avg.lot$Verb,
main = "Average Length of Themes",
xlab = "verbs", ylab = "LengthOfTheme", las = 2)
#4
contingency.table = xtabs(~ AnimacyOfTheme + AnimacyOfRec, data = verbs)
contingency.table
# Part I
#1
# Calculate the two boundaries in the distribution that
# help indicate a 2.5% chance in the upper and lower tails.
qnorm(0.025, mean = 550, sd = 25)
qnorm(0.975, mean = 550, sd = 25)
#2
# Randomly sample 10,000 values from the same normal distribution
# from (1) and calculate the proportion of values that are higher
# or lower than the two boundaries
set.seed(100)
nums = 1:10000
dist.norm = rnorm(n = nums, mean = 550, sd = 25)
randist.norm = as.data.frame(sample(x = dist.norm, size = 10000))
colnames(randist.norm) = "values"
#head(randist.norm)
head(randist.norm)
lower.dist.norm = as.data.frame(ifelse(test = randist.norm$values <= 501.0009,
yes = "TRUE", no = "FALSE"))
colnames(lower.dist.norm) = "TRUEFALSE"
higher.dist.norm = as.data.frame(ifelse(test = randist.norm$values >= 598.9991,
yes = "TRUE", no = "FALSE"))
colnames(higher.dist.norm) = "TRUEFALSE"
higher.dist.norm$TRUEFALSE == "TRUE"
matchcondition.lower = sum(lower.dist.norm$TRUEFALSE == "TRUE")
matchcondition.higher = sum(higher.dist.norm$TRUEFALSE == "TRUE")
proportion = (matchcondition.lower + matchcondition.higher)/nrow(randist.norm)
proportion
#3
# Generate the density plot of the random sample in (2) with the vertical lines
# that mark the two boundaries.
randist.norm.den = density(randist.norm$values)
plot(randist.norm.den, main = "Random Sample in Normal Distribution")
abline(v = mean(randist.norm$values), col = "red", lwd = 1.5)
abline(v = mean(randist.norm$values) + sd(randist.norm$values)*1.96, col = "orange", lwd = 1.5, lty = 2)
abline(v = mean(randist.norm$values) - sd(randist.norm$values)*1.96, col = "orange", lwd = 1.5, lty = 2)
# Part II
library(languageR)
head(verbs)
#1
# Generate a data frame that represents the average length of the theme of
# each verb.
verbs.avg.lot = aggregate(LengthOfTheme ~ Verb, FUN = mean, data = verbs)
#2
# Sort verbs.avg.lot based on the average length of the theme in increasing order
# and extract a data subset from verbs.ord that includes only the Top-20
# average theme lengths.
LengthofTheme.ord = order(verbs.avg.lot$LengthOfTheme, decreasing = F)
verbs.ord = verbs.avg.lot[LengthofTheme.ord,]
verbs.sub = head(verbs.ord, 20)
#3
# Generate an informative bar plot.
verbs.avg.lot.table = table(verbs.avg.lot$LengthOfTheme, verbs.avg.lot$Verb)
barplot(height = verbs.avg.lot$LengthOfTheme, names.arg = verbs.avg.lot$Verb, ylim = c(0, 4),
main = "The average length of the theme of verbs", xlab = "Verb",
ylab = "Length", las = 2)
#4
# Create a 2 × 2 contingency table with the original data frame verbs.
xtabs(formula = ~ AnimacyOfTheme + AnimacyOfRec, data = verbs)
#part I
#Task 1
dist.mean <- 550
dist.sd <- 25
bound.u <- dist.mean + 2 * dist.sd
bound.l <- dist.mean - 2 * dist.sd
#counting 2 sd above and below the mean
#Task 2
set.seed(666)
nd.ran <- rnorm( n = 10000 , mean = 550 , sd =25 )
#save random numbers as vector in nd.ran
nd.ifelse <- ifelse(nd.ran >= bound.u | nd.ran <= bound.l, TRUE, FALSE)
# Create a new vector that contains the results  using ifelse
sum(nd.ifelse)#number of values matched
proportion <- sum(nd.ifelse) / length(nd.ran)
#Task 3
nd.den = density(nd.ran)
plot(nd.den, main = "Density Plot of Reaction Times")
abline(v=bound.u, col="blue", lwd = 1.5, lty=2)
abline(v=bound.l, col="blue", lwd = 1.5, lty=2)
#drawing two lines: 2 standard deviation above/below the mean
# Part II
# Task 1
library(languageR)
head(verbs)
verbs.avg.lot <- aggregate(LengthOfTheme ~ Verb, FUN = mean, data = verbs)
# Task 2
verbs.ord <- order(verbs.avg.lot$LengthOfTheme, decreasing = F)
verbs.sub = verbs.ord[1:20]
verbs.sub = verbs.ord[1:20]
#Task 3
LengthOfTheme.bar <- verbs.avg.lot$LengthOfTheme
verb.bar <- verbs.avg.lot$Verb
# extract the LengthOfTheme values and Verb from verbs.avg.lot seperately
# Create the bar plot with the extracted data
barplot(height = LengthOfTheme.bar, names.arg = verb.bar,
ylim= c(0,3),
las = 2,
main = "Response Time of Verbs",
ylab = "Length of Theme", xlab = "Verbs")
# Task 4
xtabs(formula = ~AnimacyOfRec + AnimacyOfTheme , data = verbs)
#Part 1
#1
# Give parameters
mean.RT = 550
sd.RT = 25
# Calculate the boundaries for 2.5% in the upper and lower tails
lower.boundary = qnorm(0.025, mean = mean.RT, sd = sd.RT)
upper.boundary = qnorm(0.975, mean = mean.RT, sd = sd.RT)
lower.boundary
upper.boundary
#2
# Set seed
set.seed(123)
# Randomly sample 10,000 values
sample.values = rnorm(10000, mean = mean.RT, sd = sd.RT)
# Check if values are above the upper boundary or below the lower boundary
up.low = sample.values < lower.boundary | sample.values > upper.boundary
# Calculate the number of values that match the condition
num = sum(up.low)
#calculate the proportion.
proportion = num / length(sample.values)
# Density plot
plot(density(sample.values), main = "Reaction Times(Density)", xlab = "Reaction Time (ms)")
abline(v = lower.boundary, col = "blue", lty = 2)
abline(v = upper.boundary, col = "red", lty = 2)
legend(x = "bottom",cex = 0.6, legend = c("Lower Boundary", "Upper Boundary"), col = c("blue", "red"), lty = 2)
#Part 2
library(languageR)
head(verbs)
#1
#generate a data frame that represents the average length of the theme of
#each verb.
verbs.avg.lot = aggregate(LengthOfTheme ~ Verb, data = verbs, FUN = mean)
#2
verbs.ord = verbs.avg.lot[order(verbs.avg.lot$LengthOfTheme), ]
verbs.sub = head(verbs.ord, 20)
verbs.sub
#3
# generate an informative bar plot
verbs.avg.lot.1 = verbs.avg.lot[verbs.avg.lot$LengthOfTheme > 0, ]
barplot(height = verbs.avg.lot.1$LengthOfTheme, names.arg = verbs.avg.lot.1$Verb, las = 2,
ylim = c(0, 3),
main = "Average Length of Themes for Each Verb(Length>0)",
xlab = "Verbs", ylab = "Average Length of Theme", col = "orange")
# Create a 2x2 contingency table
contingency.table = table(verbs$AnimacyOfTheme, verbs$AnimacyOfRec)
contingency.table
library(languageR)
head(english)
head(dative)
?english
head(lexdec)
levels(lexdec$NativeLanguage)
cor(RT ~ FreqSingular, data = lexdec)
cor(lexdec$FreqSingular, lexdec$RT)
cor.test(lexdec$FreqSingular, lexdec$RT)
plot(density(lexdec$RT))
tail(lexdec)
mean(c(0, 100, 91, 92.5, 82))
mean(c(87, 93, 50.5, 81.5, 56.5))
mean(c(94, 92.5, 63.5, 82.5, 76))
library(languageR)
head(lexdec)
plot(lexdec$Frequency, lexdec$RT)
cor.test(lexdec$Frequency, lexdec$RT)
library(languageR)
head(lexdec)
plot(density(lexdec$Frequency))
plot(density(lexdec$RT))
lexdec.sub = subset(lexdec, RT <= mean(RT) + 2.5 * sd(RT) & RT >= mean(RT) - 2.5 * sd(RT))
plot(density(lexdec$Frequency))
plot(density(lexdec$RT))
plot(density(lexdec.sub$Frequency))
plot(density(lexdec.sub$RT))
plot(lexdec.sub$Frequency, lexdec.sub$RT)
lexdec.sub$Frequency.z = (lexdec.sub$Frequency - mean(lexdec.sub$Frequency))/sd(lexdec.sub$Frequency)
lexdec.sub$RT.z = (lexdec.sub$RT - mean(lexdec.sub$RT))/sd(lexdec.sub$RT)
plot(lexdec.sub$Frequency.z, lexdec.sub$RT.z)
sp = sum(lexdec.sub$Frequency.z * lexdec.sub$RT.z)
freq.rt.r = sp / (nrow(lexdec.sub) - 2)
freq.rt.r
cor(lexdec.sub$Frequency.z * lexdec.sub$RT.z)
cor(lexdec.sub$Frequency.z, lexdec.sub$RT.z)
freq.rt.r = sp / (nrow(lexdec.sub) - 1)
cor(lexdec.sub$Frequency.z, lexdec.sub$RT.z)
freq.rt.t = freq.rt.r / sqrt((1 - freq.rt.r ^ 2) / (nrow(lexdec.sub) - 2))
lower.p = pt(q = t, df = nrow(lexdec.sub - 2))
lower.p = pt(q = t, df = nrow(lexdec.sub) - 2)
lower.p = pt(q = freq.rt.t, df = nrow(lexdec.sub) - 2)
lower.p * 2
cor.test(lexdec.sub$Frequency.z, lexdec.sub$RT.z)
lexdec.rt = lexdec$RT
lexdec.rt = lexdec$RT
lexdec.rt.ord = lexdec.rt[order(lexdec.rt)]
head(lexdec.rt.ord)
rt.median = median(lexdec.rt.ord)
rt.1q.pos = round(length(lexdec.rt) * 1/4)
rt.3q.pos = round(length(lexdec.rt) * 3/4)
rt.iqr = IQR(lexdec.rt.ord)
rt.1q = lexdec.rt[rt.1q.pos]
rt.3q = lexdec.rt[rt.3q.pos]
outliers = lexdec.rt.ord < rt.1q.whisker | lexdec.rt.ord > rt.3q.whisker
rt.1q.whisker = rt.1q - 1.5 * rt.iqr
rt.3q.whisker = rt.3q + 1.5 * rt.iqr
outliers = lexdec.rt.ord < rt.1q.whisker | lexdec.rt.ord > rt.3q.whisker
rt.median = median(lexdec.rt.ord)
rt.1q.pos = round(length(lexdec.rt.ord) * 1/4)
rt.3q.pos = round(length(lexdec.rt.ord) * 3/4)
rt.1q = lexdec.rt.ord[rt.1q.pos]
rt.3q = lexdec.rt.ord[rt.3q.pos]
rt.iqr = IQR(lexdec.rt.ord)
rt.1q.whisker = rt.1q - 1.5 * rt.iqr
rt.3q.whisker = rt.3q + 1.5 * rt.iqr
outliers = lexdec.rt.ord < rt.1q.whisker | lexdec.rt.ord > rt.3q.whisker
lexdec.rt.ord[outliers]
4/49
source("https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/main/courseUtil.R")
# Figure 2
sl.rep.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterSim.csv")
sl.rep.avg = aggregate(rF3 ~ Subject + Condition, FUN = mean, data = sl.rep.sim)
library(ggplot2)
sl.rep.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterSim.csv")
sl.rep.avg = aggregate(rF3 ~ Subject + Condition, FUN = mean, data = sl.rep.sim)
head(sl.rep.sim)
# Figure 2
sl.rep.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterSim.csv")
sl.rep.avg = aggregate(rF3 ~ Subject + Group, FUN = mean, data = sl.rep.sim)
ggplot(data = sl.rep.sim, mapping = aes(x = Group, y = rF3, fill = Condition)) +
geom_boxplot(position = "dodge2") +
scale_y_continuous(limits = c(1800, 2800)) +
labs(title = "Figure 2. L2 English retroflex produced by L1 Japanese Speakers", x = "Pedagogical Condition",
y = "F3 in Hz (averaged within subjects", subtitle = "Simulated data") +
theme_bw()
# Figure 2
sl.rep.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterSim.csv")
sl.rep.avg = aggregate(rF3 ~ Subject + Group, FUN = mean, data = sl.rep.sim)
ggplot(data = sl.rep.sim, mapping = aes(x = Group, y = rF3, fill = Group)) +
geom_boxplot(position = "dodge2") +
scale_y_continuous(limits = c(1800, 2800)) +
labs(title = "Figure 2. L2 English retroflex produced by L1 Japanese Speakers", x = "Pedagogical Condition",
y = "F3 in Hz (averaged within subjects", subtitle = "Simulated data") +
theme_bw()
sl.rep.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterSim.csv")
sl.rep.avg = aggregate(rF3 ~ Subject + Group, FUN = mean, data = sl.rep.sim)
ggplot(data = sl.rep.sim, mapping = aes(x = Group, y = rF3, fill = Group)) +
geom_boxplot(position = "dodge2") +
scale_y_continuous(limits = c(1800, 3000)) +
labs(title = "Figure 2. L2 English retroflex produced by L1 Japanese Speakers", x = "Pedagogical Condition",
y = "F3 in Hz (averaged within subjects", subtitle = "Simulated data") +
theme_bw()
sl.rep.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterRepSim.csv")
sl.rep.avg = aggregate(rF3 ~ Subject + Group, FUN = mean, data = sl.rep.sim)
sl.rep.sim = loadCourseCSV(2024, "5_ANOVA", "SaitoLysterRepSim.csv")
sl.rep.avg = aggregate(rF3 ~ Subject + Condition, FUN = mean, data = sl.rep.sim)
ggplot(data = sl.rep.sim, mapping = aes(x = Condition, y = rF3, fill = Condition)) +
geom_boxplot(position = "dodge2") +
scale_y_continuous(limits = c(1800, 2800)) +
labs(title = "Figure 2. L2 English retroflex produced by L1 Japanese Speakers", x = "Pedagogical Condition",
y = "F3 in Hz (averaged within subjects", subtitle = "Simulated data") +
theme_bw()
# Figure 4
chen.sample = loadCourseCSV(2024, "5_ANOVA", "Chen2020Sample.csv")
chen.avg = aggregate(Accept ~ participant + Group + InitialTone, FUN = mean, data = chen.sample)
ggplot(chen.avg, aes(x = Group, y = Accept, fill = InitialTone)) +
geom_boxplot() + scale_y_continuous(limits = c(0, 1)) +
labs(title = "Figure 4. Acceptance Probability by Initial Tone across Groups", x = "Group",
y = "Acceptance Probability (averaged within subjects)", fill = "Initial Tone",
subtitle = "Source: Chen (2020)") + theme_bw()
# Figure 7
Myers.withNB = loadCourseCSV(2024, "6_Regression", "MyersWithNB.csv")
ggplot(data = Myers.withNB, mapping = aes(x = NB, y = Response)) +
geom_point(color = "grey40", alpha = .7) +
geom_smooth(method = "lm", color = "red", lwd = 1.5) +
labs(title = "Figure 7. Wordlikeness responses by phonological similarity",
x = "Number of phonologically similar words",
y = "Response (0 = No, 1 = Yes)", subtitle = "Linear Regression") +
scale_y_continuous(n.breaks = 2) +
theme_bw()
# Figure 10
chen.sample = loadCourseCSV(2024, "6_Regression", "Chen2020Sample.csv")
chen.sample$InitialTone_Fac = as.factor(chen.sample$InitialTone)
chen.sample$Group_Fac = as.factor(chen.sample$Group)
contrasts(chen.sample$InitialTone_Fac) = contr.sum(2)
colnames(contrasts(chen.sample$InitialTone_Fac)) = levels(chen.sample$InitialTone_Fac)[2]
contrasts(chen.sample$Group_Fac) = contr.sum(2)
colnames(contrasts(chen.sample$Group_Fac)) = levels(chen.sample$Group_Fac)[2]
chen.glm = glm(Accept ~ InitialTone_Fac * Group_Fac, family = "binomial",
data = chen.sample)
# The effect() function is from the effects package
chen.ef = as.data.frame(effect("InitialTone_Fac:Group_Fac", chen.glm))
library(effects)
# Figure 10
chen.sample = loadCourseCSV(2024, "6_Regression", "Chen2020Sample.csv")
chen.sample$InitialTone_Fac = as.factor(chen.sample$InitialTone)
chen.sample$Group_Fac = as.factor(chen.sample$Group)
contrasts(chen.sample$InitialTone_Fac) = contr.sum(2)
colnames(contrasts(chen.sample$InitialTone_Fac)) = levels(chen.sample$InitialTone_Fac)[2]
contrasts(chen.sample$Group_Fac) = contr.sum(2)
colnames(contrasts(chen.sample$Group_Fac)) = levels(chen.sample$Group_Fac)[2]
chen.glm = glm(Accept ~ InitialTone_Fac * Group_Fac, family = "binomial",
data = chen.sample)
# The effect() function is from the effects package
chen.ef = as.data.frame(effect("InitialTone_Fac:Group_Fac", chen.glm))
ggplot(data = chen.ef, mapping = aes(x = InitialTone_Fac, y = fit, group = Group_Fac, color = Group_Fac)) +
geom_line(stat = "identity", lwd = 1.5) + geom_point(stat = "identity", size = 4) +
geom_errorbar(mapping = aes(ymax = upper, ymin = lower), lwd = 1.5, width = .1) +
labs(title = "Figure 10. Logistic Regression for Chen (2020) Sample",
subtitle = "Accept ~ Initial Tone x Group", x = "Initial Tone",
caption = "Error bars = SE",
y = "Fitted Probability of Acceptance", color = "Group") + theme_bw()
pop = c(rep(0, 5), rep(1, 5)) 	# 我們虛無假設的母體
pop 					# 看看母體長怎樣
all.samps = combn(pop, 5) 	# 從母體中生成所有5個樣本的集合並放在一欄中
all.samps
all.ramps[2]
all.samps[2]
all.samps[,2]
all.samps[1:2]
all.samps[1:2,]
all.samps[1:5, 1:2]
all.samps[1:5, 1:3]
all.samps[1:5, 1:4]
setwd("D:/OneDrive - NTHU/Academic Works/NTHU/Courses/Language and Statistics in R/2024 Fall/Assignments/Assignment VIII")
setwd("D:/OneDrive - NTHU/Academic Works/NTHU/Courses/Language and Statistics in R/GitHub/Statistics_in_R/2024/6_Regression")
