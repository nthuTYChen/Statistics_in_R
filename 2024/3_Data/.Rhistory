#   the dots on the lift end is nearer to the Q-Q line.
### Task V
# square all the data in dist.sample
dist.sample.sqrt <- sqrt(dist.sample)
qqnorm(dist.sample.sqrt, main = "squared sample")
qqline(dist.sample.sqrt, col="red")
### Task I
head(norm.sample)
norm.ordered <- order(norm.sample)
norm.median <- 0.5*(norm.sample[norm.ordered[500]] + norm.sample[norm.ordered[501]])
norm.mean <- sum(norm.sample)/1000
norm.SD <- sqrt(var(norm.sample))
norm.median
norm.mean
norm.SD
### Task II
# Get the extreme values (as Boolean vector)
# The extreme values on the right end
norm.Rextreme <- norm.sample-(norm.mean+norm.SD)>0
# The extreme values on the left end
norm.Lextreme <- (norm.mean-norm.SD)-norm.sample>0
# Calculate how much extreme numbers are in the whole data set
length(norm.sample[norm.Rextreme])
length(norm.sample[norm.Lextreme])
85*.95
source("https://lngproc.fl.nthu.edu.tw/statisticsR/Week2/AssignmentII.R")
# Part I ####
# Task I
head(dist.sample)
# Task II
dens.dist.sample = density(dist.sample)
plot(dens.dist.sample, main = "Distribution: dist.sample")
abline(v = mean(dist.sample), col = "red", lwd = 1.5) # mean
abline(v = mean(dist.sample) - sd(dist.sample), col = "green", lwd = 1.5, lty = 2) # -1 SD
abline(v = mean(dist.sample) + sd(dist.sample), col = "green", lwd = 1.5, lty = 2) # +1 SD
# Task III
qqnorm(dist.sample, main = "Normal Q-Q Plot: dist.sample")
qqline(dist.sample, col = "red", lwd = 1.5)
# Task V
dist.sample.new = sqrt(dist.sample)
qqnorm(dist.sample.new, main = "Normal Q-Q Plot: dist.sample.new")
qqline(dist.sample.new, col = "red", lwd = 1.5)
# Part II ####
# Task I
head(norm.sample)
# median
ord.tag = order(norm.sample, decreasing = F) # generate vector for position number
norm.sample.ord = norm.sample[ord.tag]
median.norm.sample = (norm.sample.ord[500] + norm.sample.ord[501]) / 2
median.norm.sample # Ans
# mean
mean.norm.sample = sum(norm.sample) / 1000
mean.norm.sample # Ans
# SD
ss.norm.sample = sum((norm.sample - mean.norm.sample) ^ 2) # sum of squares
var.norm.sample = ss.norm.sample / (1000 - 1) # variance
sd.norm.sample = sqrt(var.norm.sample) # sd
sd.norm.sample # Ans
# Task II
norm.sample.extreme =
norm.sample[norm.sample < (mean.norm.sample - 1.96 * sd.norm.sample) |
norm.sample > (mean.norm.sample + 1.96 * sd.norm.sample)]
length(norm.sample.extreme) # 54
source("https://lngproc.fl.nthu.edu.tw/statisticsR/Week2/AssignmentII.R")
#part1
#task1
dist.sample
head(dist.sample)
#task2
dist.sample.den = density(dist.sample)
plot(dist.sample.den, main= "sample")
abline(v = mean(dist.sample), col ="red", lwd =1.5)
dist.sample.diff = dist.sample - mean(dist.sample)
ss.sample = sum(dist.sample.diff^2)
ss.sample
sd.norm = sqrt(ss.sample/ (length(dist.sample.diff)-1))
#task3
qqnorm(dist.sample, main= "Q-Qplot: dist.sample")
qqline(dist.sample, col="red")
#task4
#the plot is skewR
#task5
dist.sample.new = dist.sample ^ 1/3
abline(v = mean(dist.sample.new), col ="blue", lwd =1.5)
qqnorm(dist.sample.new, main= "Q-Qplot: dist.sample.new")
qqline(dist.sample.new, col="blue")
norm.sample
head(norm.sample)
norm.sample.df = as.data.frame(norm.sample)
head(norm.sample.df)
colnames(norm.sample.df) = c("Count")
sum(norm.sample.df$Count)
order(norm.sample$Count, decreasing = T)
head(order(norm.sample))
order(norm.sample.df$Count, decreasing = T)
count.des.ord= order(norm.sample.df$Count, decreasing= T)
norm.sample.ord = norm.sample.df[count.des.ord, ]
head(norm.sample.ord)
summary(norm.sample)
source("https://lngproc.fl.nthu.edu.tw/statisticsR/Week2/AssignmentII.R")
# Part I
# Task I
head(dist.sample)
# Task II
dist.sample.den = density(dist.sample)
plot(dist.sample.den, main = "Sample Distribution")
abline(v = mean(dist.sample), col = "red", lwd = 1.5)
abline(v = mean(dist.sample) + sd(dist.sample), col = "green", lwd = 1.5, lty = 2)
abline(v = mean(dist.sample) - sd(dist.sample), col = "green", lwd = 1.5, lty = 2)
# Task III
qqnorm(dist.sample, main = "Sample Q-Q Plot")
qqline(dist.sample, col = "red")
# Task IV
# The plot suggests right skewness.
# The right of the peak has a longer tail.
# Also in the Q-Q plot the higher end of the distribution
# are far away from the Q-Q line than the lower end.
# Task V
dist.sample.new = dist.sample ^ 0.5
qqnorm(dist.sample.new, main = "New Sample Q-Q Plot")
qqline(dist.sample.new, col = "red")
# Part II
# Task I
norm.des.ord = order(norm.sample, decreasing = T)
norm.sample.order = norm.sample[norm.des.ord]
norm.sample.median = (norm.sample.order[500] + norm.sample.order[501]) / 2
norm.sample.median
norm.sample.mean = sum(norm.sample) / length(norm.sample)
norm.sample.diff = norm.sample - norm.sample.mean
ss.norm.sample = sum(norm.sample.diff^2)
sd.norm.sample = sqrt(ss.norm.sample / (length(norm.sample.diff) - 1))
# Task II
norm.sample[norm.sample > mean(norm.sample) + 1.96 * sd(norm.sample)]
norm.sample[norm.sample < mean(norm.sample) - 1.96 * sd(norm.sample)]
length(norm.sample[norm.sample > mean(norm.sample) + 1.96 * sd(norm.sample)]) +
length(norm.sample[norm.sample < mean(norm.sample) - 1.96 * sd(norm.sample)])
# The proportion of the extreme values is 5.4%, which is close to 5%.
# So this group of children are properly sampled from the population.
#Part 1
#Task I
source("http://lngproc.fl.nthu.edu.tw/statisticsR/Week2/AssignmentII.R")
head(dist.sample)
#Task II
dist.sample.den=density(dist.sample)
plot(dist.sample.den, main = "dist.sample with mean and +-1SD")
abline(v=mean(dist.sample), col="red", lwd= 1.5)
abline(v= mean(dist.sample)+sd(dist.sample), col= "green", lwd=1.5, lty=2)
abline(v= mean(dist.sample)-sd(dist.sample), col= "green", lwd=1.5, lty=2)
#Task III
qqnorm(dist.sample, main = "Q-Q Plot: dist.sample")
qqline(dist.sample, col="red")
#Task IV
#The plots suggests right skewness of the data distribution since they
#are obvious deviations from the ideal correlation line toward the end of
#the distribution, suggesting that some numbers are higher than normal
#distribution.
#Task V
dist.sample.new=sqrt(dist.sample)
qqnorm(dist.sample.new, main="Q-Q Plot: dist.sample.new")
qqline(dist.sample.new, col="red")
#Part II
#Task I
head(norm.sample)
order(norm.sample)
#median
count.des.ord=order(norm.sample, decreasing = T)
norm.sample.ord=norm.sample[count.des.ord]
norm.sample.ord[500]
#mean
norm.sample.mean=sum(norm.sample)/length(norm.sample)
#sd
norm.sample.diff= norm.sample-norm.sample.mean
ss.norm=sum(norm.sample.diff^2)
sd.norm=sqrt(ss.norm/(length(norm.sample.diff)-1))
#Task II
length(pnorm(norm.sample, mean = 0, sd=2, lower.tail = FALSE))
length(pnorm(norm.sample, mean = 0, sd=2, lower.tail = TRUE))
#this group of children are properly sampled from the population because
#the number of values are both 1000 in upper tail and lower tail.
sd.norm
norm.sample.mean
norm.sample.mean
sd(norm.sample)
s.huh = sum(norm.sample ^ 2) /
(length(norm.sample) - 1) - mean(norm.sample) ^ 2
s.huh
s.huh = sqrt(sum(norm.sample ^ 2) /
(length(norm.sample) - 1) - mean(norm.sample) ^ 2)
s.huh
library(languageR)
head(durationsOnt)
library(reshape)
head(melt(durationsOnt, Sex ~ DurationPrefixNasal))
head(melt(Sex, durationsOnt[c("Sex", "DurationPrefixVowel", "DurationPrefixNasal", "DurationalPrefixPlosive")])
)
?melt
head(melt(durationsOnt[c("Sex", "DurationPrefixVowel", "DurationPrefixNasal", "DurationalPrefixPlosive")], Sex))
head(melt(durationsOnt[c("Sex", "DurationPrefixVowel", "DurationPrefixNasal", "DurationPrefixPlosive")], Sex))
head(melt(durationsOnt[c("Sex", "DurationPrefixVowel", "DurationPrefixNasal", "DurationPrefixPlosive")], id.vars = "Sex"))
levels(as.factor(durationsOnt$Word))
Myers.sample = loadCourseCSV("Week3-4", "Myers_2015_Sample.csv")
# Remove responses with an invalid RT and log-transform the rest of the RTs
Myers.resp = subset(Myers.sample, RT > 0)
Myers.resp$logRT = log(Myers.resp$RT)
# Divide the sample data set into two subsets by Session
Myers.s1 = subset(Myers.sample, Session == 1)
Myers.s2 = subset(Myers.sample, Session == 2)
source("https://lngproc.fl.nthu.edu.tw/statisticsR/courseUtil.R")
Myers.sample = loadCourseCSV("Week3-4", "Myers_2015_Sample.csv")
# Remove responses with an invalid RT and log-transform the rest of the RTs
Myers.resp = subset(Myers.sample, RT > 0)
Myers.resp$logRT = log(Myers.resp$RT)
# Divide the sample data set into two subsets by Session
Myers.s1 = subset(Myers.sample, Session == 1)
Myers.s2 = subset(Myers.sample, Session == 2)
summary(Myers.s1)
summary(Myers.s1$RT)
summary(Myers.s1$logRT)
Myers.s1 = subset(Myers.resp, Session == 1)
Myers.s2 = subset(Myers.resp, Session == 2)
# Get the distributional properties of RT for each subset
summary(Myers.s1$logRT)
summary(Myers.s2$logRT)
7.377 - 6.229
7.416 - 6.394
7.377 + 1.148 * 1.5
6.229 - 1.148 * 1.5
7.416 + 1.022 * 1.5
6.394 â€“ 1.022 * 1.5
6.394 - 1.022 * 1.5
source("https://lngproc.fl.nthu.edu.tw/statisticsR/courseUtil.R")
jabberwocky.wd = loadCourseCSV("Week3-4", "jabberwocky_words.txt")
jabberwocky.table = table(jabberwocky.wd$Word)
jabberwocky.df = as.data.frame(jabberwocky.table)
colnames(jabberwocky.df) = c("Word", "Count")
jabberwocky.wordCat = loadCourseCSV("Week3-4", "jabberwocky_words_cat.csv")
jabberwocky.all = merge(jabberwocky.ord, jabberwocky.wordCat, by = "Word")
source("https://lngproc.fl.nthu.edu.tw/statisticsR/courseUtil.R")
jabberwocky.wd = loadCourseCSV("Week3-4", "jabberwocky_words.txt")
jabberwocky.table = table(jabberwocky.wd$Word)
jabberwocky.df = as.data.frame(jabberwocky.table)
colnames(jabberwocky.df) = c("Word", "Count")
jabberwocky.wordCat = loadCourseCSV("Week3-4", "jabberwocky_words_cat.csv")
jabberwocky.all = merge(jabberwocky.df, jabberwocky.wordCat, by = "Word")
xtabs(Count ~ Real + POS, data = jabberwocky.all)
xtabs(~ Real + POS, data = jabberwocky.all)
xtabs(formula = ~ Real, data = jabberwocky.all)
# Second attempt: two independent variables; parameter names are omitted
xtabs(formula = ~ Real + POS, jabberwocky.all)
POS
Real cont func
N   43    0
Y   65   56
jabberwocky.xtabs = xtabs(formula = ~ Real + POS, data = jabberwocky.all)
# Get the total count by submitting the contingency table to sum()
jabberwocky.sum = sum(jabberwocky.xtabs)
jabberwocky.sum
jabberwocky.xtabs / jabberwocky.sum
jabberwocky.perc = round(x = jabberwocky.xtabs / jabberwocky.sum * 100,
digit = 1)
jabberwocky.perc
mosaicplot(x = jabberwocky.xtabs, main = "Jabberwocky Word Types Distribution",
xlab = "Real Word", ylab = "Part of Speech",
color = c("white", "grey40"))
setwd("D:/OneDrive/Documents/Academic Works/NTHU/Courses/Language and Statistics in R/GitHub/Statistics_in_R/Week3-4")
library(languageR)
boxplot(DurationOfPrefix, data = durationsOnt)
boxplot(DurationOfPrefix ~, data = durationsOnt)
boxplot(durationsOnt$DurationOfPrefix)
boxplot(durationsOnt$DurationOfVowel)
head(durationsOnt)
boxplot(durationsOnt$DurationPrefixVowel)
boxplot(durationsOnt$Frequency)
boxplot(durationsOnt$SpeechRate)
boxplot(durationsOnt$SpeechRate, ylim = c(0, 10))
boxplot(durationsOnt$SpeechRate, ylim = c(0, 10), outline = F)
# Load a script from my server, so you can simply use loadCourseCSV()
# for the rest of this course to help load some datasets for this course.
# Note that this function is not a base function, so you always have to
# load this script in order to use it.
source("https://lngproc.hss.nthu.edu.tw/statisticsR/courseUtil.R")
# Load wordRT.csv from the Week4-5 folder from my course server,
# and save the data frame as wordRT
wordRT = loadCourseCSV(week = "Week4-5", file = "wordRT.csv")
# wordRT itself is not a proper data frame, because one row represents
# multiple observations from different speakers. Also see Week 4-5 handout
# for detailed explantion.
# Try to "reshape" the data frame into a data frame in which each row represents
# one observation.
# So we need to use some function from the "reshape" package.
library(reshape)
# Load a script from my server, so you can simply use loadCourseCSV()
# for the rest of this course to help load some datasets for this course.
# Note that this function is not a base function, so you always have to
# load this script in order to use it.
source("https://lngproc.hss.nthu.edu.tw/statisticsR/courseUtil.R")
# Load wordRT.csv from the Week4-5 folder from my course server,
# and save the data frame as wordRT
wordRT = loadCourseCSV(week = "Week4-5", file = "wordRT.csv")
# wordRT itself is not a proper data frame, because one row represents
# multiple observations from different speakers. Also see Week 4-5 handout
# for detailed explantion.
# Try to "reshape2" the data frame into a data frame in which each row represents
# one observation.
# So we need to use some function from the "reshape" package.
library(reshape2)
# Load the Jabberwocky corpus from my server
jabberwocky.wd = loadCourseCSV("Week4-5", "jabberwocky_words.txt")
# Each observation is one word "token" (i.e., individual word) from
# the poem, so the number of rows in the data frame is equal to the number
# of individual word tokens in the poem.
nrow(jabberwocky.wd)
# Convert the Word vector into a "frequency table", which shows how many times
# each unique word type in the Word column occurs in the corpus.
jabberwocky.table = table(jabberwocky.wd$Word)
# Covert the frequency table into a data frame, in which each row represents
# one observation of a unique word type
jabberwocky.df = as.data.frame(jabberwocky.table)
# Change the column names to those that make more sense
colnames(jabberwocky.df) = c("Word", "Count")
# The number of rows = the number of unique word types in this data frame
nrow(jabberwocky.df)
# The sum of the counts are still the total number of word tokens in the corpus
sum(jabberwocky.df$Count)
# Get row numbers based on the values in Count in decreasing order.
order(jabberwocky.df$Count, decreasing = TRUE)
# The first row number returned above is 71, so check if the 71st row of the data frame
# is really the word with the highest count.
jabberwocky.df[71,]
# Store the ordered row numbers in another vector
count.des.ord = order(jabberwocky.df$Count, decreasing = TRUE)
# Put the ordered row numbers to specify the rows of a data frame, so you get a new
# data frame arranged based on the ordered row numbers.
jabberwocky.ord = jabberwocky.df[count.des.ord, ]
# Load another data frame with the word category information for each word type
# in the corpus
jabberwocky.wordCat =
loadCourseCSV("Week4-5", "jabberwocky_words_cat.csv")
# Check if each value in POS is NA, and the result is a vector of TRUEs and FALSEs.
pos.na = is.na(jabberwocky.wordCat$POS)
# sum() converts TRUEs into 1s and FALSES into 0s and add all the values together,
# so the output is the total number of NAs in POS.
sum(pos.na)
# Put TRUEs and FALSEs to specify the rows of a data frame, so R selects only the
# "TRUE" rows for you.
jabberwocky.wordCat[pos.na, ]
# Adding ! reverses TRUEs and FALSEs.
jabberwocky.wordCat[!pos.na, ]
# Merge the data frame with counts into the data frame with the word type info
# based on the Word column.
jabberwocky.all =
merge(jabberwocky.ord, jabberwocky.wordCat, by = "Word")
# Sort the Count values by the two levels in Real (Y and N), and add all the values
# in Count together for each level so you get the total number of real word and
# non-word tokens.
xtabs(formula = Count ~ Real, data = jabberwocky.all)
# Count the number of each of the two levels in Real (Y and N), so you get the unique
# types of real words and nonwords.
xtabs(formula = ~ Real, data = jabberwocky.all)
# A get a 2 x 2 Contingency Table that show the number of each of the four possible
# combinations between the two levels in Real (Y and N) and the two levels in POS (Cont and Func)
jabberwocky.xtabs =
xtabs(formula = ~ Real + POS, data = jabberwocky.all)
# Add all the numbers in the contingency table to get a total count.
jabberwocky.sum = sum(jabberwocky.xtabs)
# Divide the entire table by the sum to get the probability of each combination
jabberwocky.xtabs / jabberwocky.sum
# Get the percentages of each combination rounded to the first decimal
jabberwocky.perc =
round(x = jabberwocky.xtabs / jabberwocky.sum * 100, digit = 1)
# Load the library for the durationsOnt data frame
library(languageR)
head(durationsOnt)
# Check the distribution info for DurationOfPrefix (the duration of the ont- prefix
# in seconds)
summary(durationsOnt$DurationOfPrefix)
# Get the absolute difference between the mean and median -> very close ->
# probably a normal distribution
abs(median(durationsOnt$DurationOfPrefix) -
mean(durationsOnt$DurationOfPrefix))
# Get the absolute difference between the mean and the 1st/3rd quartile -> comparable ->
# probably symmetrical
abs(0.1167 - 0.1581)
abs(0.1581 - 0.1743)
# par() is used to set the layout of the plotting area; see Week 4-5 Handout.
par(mfrow = c(1, 2))
plot(density(durationsOnt$DurationOfPrefix),
main = "Duration of ont- Prefix (Density)")
abline(v = median(durationsOnt$DurationOfPrefix), col = "blue",
lwd = 1.5)
abline(v = mean(durationsOnt$DurationOfPrefix), col = "red",
lwd = 1.5)
qqnorm(durationsOnt$DurationOfPrefix,
main = "Duration of ont- Prefix (Q-Q Plot)")
qqline(durationsOnt$DurationOfPrefix, col = "red", lwd = 1.5)
par(mfrow = c(1, 1))
# Get the probability of observing a value in a normal distribution from
# the area in the lower tail below mean - SD * 2.5
pnorm(q = -2.5)
# Get the probability of observing a value in a normal distribution from
# the areas below mean - SD * 2.5 and above mean + SD * 2.5
pnorm(q = -2.5) * 2
# +-SD * 3 as a stricter threshold
pnorm(q = -3) * 2
# Calculate mean +- SD * 2.5 for the DurationOfPrefix distribution to
# set the boundaries for outliers.
sd.lower = mean(durationsOnt$DurationOfPrefix) -
sd(durationsOnt$DurationOfPrefix) * 2.5
sd.upper = mean(durationsOnt$DurationOfPrefix) +
sd(durationsOnt$DurationOfPrefix) * 2.5
# Judging by whether DurationOfPrefix is lower than sd.lower
# OR higher than sd.upper, you get TRUEs and FALSEs to indication
# if each value in DurationOfPrefix is an outlier
outliers = durationsOnt$DurationOfPrefix < sd.lower |
durationsOnt$DurationOfPrefix > sd.upper
# TRUEs are converted into 1s and FALSES into 0s in sum(), so you
# get the total number of outliers (only 1).
sum(outliers)
# Use TRUEs and FALSES in "outliers" to select "TRUE" rows from
# the data frame durationsOnly.
durationsOnt[outliers, ]
# The "Frequency" is in fact log-transformed (natural log)
head(durationsOnt$Frequency)
# Convert log-transformed frequencies back to raw frequencies
# with exp() since the oppsite of logs is exponents.
durationsOnt$Frequency.raw = exp(durationsOnt$Frequency)
# Visualize a right-skewed raw frequency distribution following
# the Zipf's Law (many low-frequency words and a few high-frequency words)
par(mfrow = c(1, 2))
plot(density(durationsOnt$Frequency.raw),
main = "Raw Frequency of ont- Words (Density)")
qqnorm(durationsOnt$Frequency.raw,
main = "Raw Frequency of ont- Words (Q-Q Plot)")
qqline(durationsOnt$Frequency.raw, col = "red", lwd = 1.5)
# Compare the difference between small numbers to those between big numbers
# before and after log-transformation; the distance is made equal between
# small numbers and between large numbers after log-transformation.
log(c(0.5, 1, 2, 4, 8))
log(c(200, 400, 800))
# Visualize a normal-like log frequency distribution
plot(density(durationsOnt$Frequency),
main = "Log Frequency of ont- Words (Density)")
qqnorm(durationsOnt$Frequency,
main = "Log Frequency of ont- Words (Q-Q Plot)")
qqline(durationsOnt$Frequency, col = "red", lwd = 1.5)
par(mfrow = c(1, 1))
# Categorize words based on whether its log frequency is higher
# than or equal to the median log frequency.
# ifelse() returns a vector of "yes" and "no" values based on
# the results of the "test", and the vector is stored into the
# Frequency.cat variable in durationsOnt data frame.
durationsOnt$Frequency.cat =
ifelse(test = durationsOnt$Frequency >=
median(durationsOnt$Frequency),
yes = "High", no = "Low")
# Sort DurationOfPrefix data based on the frequency category
# (High vs. Low) and apply the mean() function to calculate
# the average log frequency for each category.
aggregate(x = DurationOfPrefix ~ Frequency.cat,
FUN = mean, data = durationsOnt)
# Sort DurationOfPrefix data based on the frequency category
# (High vs. Low) and apply the sd() function to calculate
# the standard deviation of log frequency for each category.
aggregate(x = DurationOfPrefix ~ Frequency.cat,
FUN = sd, data = durationsOnt)
# Get Pearson's correlation coefficient; a small negative value
# means a negative correlation between word frequency and the
# duration of the ont- prefix.
cor(durationsOnt$Frequency, durationsOnt$DurationOfPrefix)
# Make sure that you have jabberwocky.wd loaded into your R
# Generate the frequency table of jabberwocky again.
jabberwocky.table = table(jabberwocky.wd$Word)
# Sort the frequency counts by a decreasing order
jabberwocky.table =
jabberwocky.table[order(jabberwocky.table, decreasing = T)]
# Exclude data entries with a frequency count higher than 1
# Again, the condition "jabberwocky.table > 1" gives you
# TRUEs and FALSES, and putting them into [] of a table means...?
jabberwocky.table.2 = jabberwocky.table[jabberwocky.table > 1]
# See Week 4-5 handout for the detailed explanations of the
# data visualization process below.
barplot(height = jabberwocky.table.2,
main = "Jabberwocky Word Count (Token Freq > 2)",
xlab = "", ylab = "Count", ylim = c(0, 20), las = 2)
jabberwocky.df = as.data.frame(jabberwocky.table)
colnames(jabberwocky.df) = c("Word", "Count")
# Make sure that you have loadCourseCSV() ready, too.
jabberwocky.wordCat =
loadCourseCSV("Week4-5", "jabberwocky_words_cat.csv")
jabberwocky.all =
merge(jabberwocky.df, jabberwocky.wordCat, by = "Word")
jabberwocky.xtabs = xtabs(~ Real + POS, jabberwocky.all)
mosaicplot(x = jabberwocky.xtabs,
main = "Jabberwocky Word Types Dsitribution",
xlab = "Real Word", ylab = "Part of Speech",
color = c("white", "grey40"))
boxplot(DurationOfPrefix ~ Frequency.cat, data = durationsOnt,
main = "Prefix Duration by Frequency",
xlab = "log Frequency divided by Median",
ylab = "Duration in ms")
ggplot(data = jabberwocky.2.df, mapping = aes(x = Word, y = Count)) + geom_bar()
library(ggplot2)
ggplot(data = jabberwocky.2.df, mapping = aes(x = Word, y = Count)) + geom_bar()
jabberwocky.2.df = as.data.frame(jabberwocky.table.2)
colnames(jabberwocky.2.df) = c("Word", "Count")
ggplot(data = jabberwocky.2.df, mapping = aes(x = Word, y = Count)) + geom_bar()
ggplot(data = jabberwocky.2.df, mapping = aes(x = Word, y = Count)) + geom_bar(stat = "identity")
ggplot(data = jabberwocky.2.df) + geom_bar(mapping = aes(x = Word, y = Count), stat = "identity")
ggplot(data = jabberwocky.2.df) + geom_bar(mapping = aes(x = Word, y = Count), stat = "identity") + coord_flip()
ggplot(data = jabberwocky.2.df) + geom_bar(mapping = aes(x = Word, y = Count), stat = "identity") + coord_flip() + scale_y_continuous(expand = c(0.1, 0.1))
ggplot(data = jabberwocky.2.df) + geom_bar(mapping = aes(x = Word, y = Count), stat = "identity") + coord_flip() + scale_y_continuous(expand = c(0.01, 0.01))
ggplot(data = jabberwocky.2.df) + geom_bar(mapping = aes(x = Word, y = Count), stat = "identity") + coord_flip() + scale_y_continuous(expand = c(0.01, 0.01), limits = c(0, 20))
