library(ggplot2)
ggplot(data = english.sub, mapping = aes(x = Len.fac, y = RTnaming)) +
geom_boxplot() +
facet_grid(~ CV)
library(languageR)
head(english)
#Task I
english.sub = subset(english, english$LengthInLetters >= 3 &
english$LengthInLetters <= 5 &
english$AgeSubject == "young")
#Task II
english.sub$Len.fac = as.factor(english.sub$LengthInLetters)
levels(english.sub$Len.fac)
#Task III
plot(density(english.sub$RTnaming),
main = "RTnaming distribution plot")
abline(v = mean(english.sub$RTnaming),col = "red")
abline(v = median(english.sub$RTnaming),col = "blue")
#Task IV
mean.RTnaming.sub = mean(english.sub$RTnaming)
sd.RTnaming.sub = sd(english.sub$RTnaming)
eng.outupper = mean.RTnaming.sub + sd.RTnaming.sub*2
eng.outlower = mean.RTnaming.sub - sd.RTnaming.sub*2
english.clean = subset(english.sub, RTnaming > eng.outlower & RTnaming < eng.outupper)
plot(density(english.clean$RTnaming),
main = "RTnaming distribution plot without outliers")
abline(v = mean(english.clean$RTnaming),col = "red")
abline(v = median(english.clean$RTnaming),col = "blue")
#The graph without outliers looks more like a normal distri
#Task V
lengthLetter.3 = subset(english.clean, LengthInLetters == "3")
lengthLetter.4 = subset(english.clean, LengthInLetters == "4")
lengthLetter.5 = subset(english.clean, LengthInLetters == "5")
mean.3 = mean(lengthLetter.3$RTnaming)
mean.4 = mean(lengthLetter.4$RTnaming)
mean.5 = mean(lengthLetter.5$RTnaming)
mean.grand = mean(english.clean$RTnaming)
ss.between = nrow(lengthLetter.3)*(mean.3 - mean.grand)^2 +
nrow(lengthLetter.4)*(mean.4 - mean.grand)^2 +
nrow(lengthLetter.5)*(mean.5 - mean.grand)^2
dfbetween = 3-1 #number of level - 1
MSbetween = ss.between / dfbetween
ss.3 = sum((lengthLetter.3$RTnaming - mean.3)^2)
ss.4 = sum((lengthLetter.4$RTnaming - mean.4)^2)
ss.5 = sum((lengthLetter.5$RTnaming - mean.5)^2)
ss.within = sum(ss.3,ss.4,ss.5)
dfwithin = length(english.clean$LengthInLetters) - 3
MSwithin = ss.within / dfwithin
F_length_RT = MSbetween / MSwithin
#Task VI
summary(aov(formula = RTnaming ~ Len.fac, data = english.clean))
#Task VII
TukeyHSD(aov(formula = RTnaming ~ Len.fac, data = english.clean))
#Task VIII
CV.aov = aov(formula = RTnaming ~ Len.fac * CV, data = english.clean)
summary(CV.aov)
#Task IX
library(ggplot2)
ggplot(english.clean, aes(x = Len.fac, y = RTnaming)) +
geom_boxplot() +
facet_grid(CV ~. ) +
xlab("Word Length") +
ylab("Reaction Time") +
ggtitle("Reaction Time of Word Naming") +
theme_bw()
library(languageR)
head(english)
#Task I
english.sub = subset(english, LengthInLetters>=3&LengthInLetters<=5&AgeSubject=="young")
#Task II
english.sub$Len.fac = as.factor(english.sub$LengthInLetters)
levels(english.sub$Len.fac)
#Task III
plot(english.sub$RTnaming, main = "RTnaming Distribution")
#Task VI
aov = aov(RTnaming ~ Len.fac, data = english.sub)
summary(aov)
#Task VIII
aov2 = aov(RTnaming ~ Len.fac*CV, data = english.sub)
summary(aov2)
library(ggplot2)
ggplot(data = aov2, aes(x =  Len.fac, y = RTnaming)) + facet_grid(~ CV) + labs(title = "Two Way Interaction of Len.fac and CV")
sum(c(75, 60, 80, 60, 80, 38, 50, 30, 10, 30) * 0.04)
sum(c(75, 60, 80, 60, 80, 50, 30, 10, 30) * 0.04)
38 * 0.3
29.6
30
25/30 * 100
library(languageR)
head(ratings)
# Part I
# Task I
# Get the numeric information about the two variables for excluding outliers
# from the two variables as usual.
sizeRate.mean = mean(ratings$meanSizeRating)
sizeRate.sd = sd(ratings$meanSizeRating)
weightRate.mean = mean(ratings$meanWeightRating)
weightRate.sd = sd(ratings$meanWeightRating)
# The joined conditions in subset() implies that an observation is included only
# when (a) its mean size rating is not an outlier AND (b) its mean weight rating
# is not an outlier either.
ratings.sub = subset(ratings,
meanSizeRating <= sizeRate.mean + 2 * sizeRate.sd &
meanSizeRating >= sizeRate.mean - 2 * sizeRate.sd &
meanWeightRating <= weightRate.mean + 2 * weightRate.sd &
meanWeightRating >= weightRate.mean - 2 * weightRate.sd)
# Divide the number of observations in "ratings.sub" by the number of observations
# in "ratings", you get the proportion of INCLUDED observations. Subtract the
# proportion from 1, you get the proportion of EXCLUDED observations.
1 - (nrow(ratings.sub) / nrow(ratings)) # 1.2%
# Task II
# Update the means and sds of the two variables for calculating the statistics
# in Pearson's correlation test using "ratings.sub". If I don't do this and just
# used the means and sds obtained from Task I, I will be getting wrong numbers
# because the numbers in Task I were from the original data frame "ratings",
# not "ratings.sub".
sizeRate.mean = mean(ratings.sub$meanSizeRating)
sizeRate.sd = sd(ratings.sub$meanSizeRating)
weightRate.mean = mean(ratings.sub$meanWeightRating)
weightRate.sd = sd(ratings.sub$meanWeightRating)
# Z-score the two variables manually, because Pearson's correlation coefficient
# is a standardized coefficient; the difference in the scale of the two
# correlated variables would be removed.
ratings.sub$size.z =
(ratings.sub$meanSizeRating - sizeRate.mean) / sizeRate.sd
ratings.sub$weight.z =
(ratings.sub$meanWeightRating - weightRate.mean) / weightRate.sd
# Calculate the sum of products by multiplying the two z-scored variables.
sizeWeight.sp = sum(ratings.sub$size.z * ratings.sub$weight.z)
# Divide the sum of products by the degree of freedom (i.e., the number of
# paired observations minus 1), we get Pearson's r.
sizeWeight.r = sizeWeight.sp / (nrow(ratings.sub) - 1) # r = 0.999
# Run a one-sample t-test assuming 0 as the population mean.
# Get the t-value with Pearson's r, r-squared, the number paired observations,
# and the degree of freedom; check formula (9) in the Week 7-9 Handout.
sizeWeight.t = sizeWeight.r /
sqrt((1 - sizeWeight.r ^ 2) / (nrow(ratings.sub) - 2)) # t = 241.14
# Get a one-tailed p-value from a t-distribution using pt(). Since t-value
# is positive, get the upper-tail p-value by setting lower.tail to FALSE.
sizeWeight.p = pt(q = sizeWeight.t, df = nrow(ratings.sub) - 2,
lower.tail = FALSE)
# Get a two-tailed p, which is smaller than .001, so our Pearson's r is
# significantly different from 0.
sizeWeight.p * 2 # p < .001
# Verify our statistics using cor.test()
cor.test(ratings.sub$meanWeightRating, ratings.sub$meanSizeRating)
# Same r, t, and p (< .001).
# We tested the correlation between mean size rating and mean weight rating in
# Pearson's correlation test, and the result suggests a significant positive
# correlation (r = .999, t(78) = 241.14, p < .001). The effect size is also
# very large (r2 = .999).
# Task III
# I probably do not have to explain too much regarding how to using the base
# plot() function to generate a scatter plot...? The only thing that is probably
# worth mentioning is that it does matter which vairable is presented on the
# x-axis or y-axis. We are not talking about which variable is explained by
# another variable, but just how two variables covary with each other.
plot(ratings.sub$size.z, ratings.sub$weight.z,
main = "Correlation between mean size and\n mean weight ratings of English nouns",
xlab = "Size Rating (z-score)", ylab = "Weight Rating (z-score)")
0.087 / 0.479
0.479 / 0.087
# Part II
# Task IV
# Get the two samples of mean size ratings based on the two classes first.
plant.size = ratings.sub[ratings.sub$Class == "plant",]$meanSizeRating
animal.size = ratings.sub[ratings.sub$Class == "animal",]$meanSizeRating
# Use var() to get the variance of the two samples.
plant.var = var(plant.size) # 0.087
animal.var = var(animal.size) # 0.181
# Calculate the F-value manually by dividing one variance by another.
# In my case, I divide the variance of the "plant" sample by that of the
# "animal" sample, and get a variance ratio of 0.479.
plant.animal.f = plant.var / animal.var # 0.182
# Get a one-tailed p-value with an F-value from an F-distribution based on two
# dfs, which are the size of the two-samples minus 1. Since my F-value is
# smaller than 1, which is the assumed population mean, my one-tailed p-value
# is from the lower tail of the F-distribution, so I don't have to set
# "lower.tail" to TRUE because by default it is true.
plant.animal.p = pf(q = plant.animal.f, df1 = length(plant.size) - 1,
df2 = length(animal.size) - 1)
plant.animal.f
0.181 / 0.087
pf(q = 2.08046, df1 = length(plant.size) - 1,
df2 = length(animal.size) - 1)
pf(q = 2.08046, df1 = length(plant.size) - 1,
df2 = length(animal.size) - 1, lower.tail = F)
var.test(plant.size, animal.size)
var.test(animal.size, plant.size)
pf(q = 2.08091, df1 = length(plant.size) - 1,
df2 = length(animal.size) - 1, lower.tail = F)
pf(q = 2.0891, df1 = length(plant.size) - 1,
df2 = length(animal.size) - 1, lower.tail = F)
library(languageR)
head(ratings)
# Part I
# Task I
# Get the numeric information about the two variables for excluding outliers
# from the two variables as usual.
sizeRate.mean = mean(ratings$meanSizeRating)
sizeRate.sd = sd(ratings$meanSizeRating)
weightRate.mean = mean(ratings$meanWeightRating)
weightRate.sd = sd(ratings$meanWeightRating)
# The joined conditions in subset() implies that an observation is included only
# when (a) its mean size rating is not an outlier AND (b) its mean weight rating
# is not an outlier either.
ratings.sub = subset(ratings,
meanSizeRating <= sizeRate.mean + 2 * sizeRate.sd &
meanSizeRating >= sizeRate.mean - 2 * sizeRate.sd &
meanWeightRating <= weightRate.mean + 2 * weightRate.sd &
meanWeightRating >= weightRate.mean - 2 * weightRate.sd)
# Divide the number of observations in "ratings.sub" by the number of observations
# in "ratings", you get the proportion of INCLUDED observations. Subtract the
# proportion from 1, you get the proportion of EXCLUDED observations.
1 - (nrow(ratings.sub) / nrow(ratings)) # 1.2%
# Task II
# Update the means and sds of the two variables for calculating the statistics
# in Pearson's correlation test using "ratings.sub". If I don't do this and just
# used the means and sds obtained from Task I, I will be getting wrong numbers
# because the numbers in Task I were from the original data frame "ratings",
# not "ratings.sub".
sizeRate.mean = mean(ratings.sub$meanSizeRating)
sizeRate.sd = sd(ratings.sub$meanSizeRating)
weightRate.mean = mean(ratings.sub$meanWeightRating)
weightRate.sd = sd(ratings.sub$meanWeightRating)
# Z-score the two variables manually, because Pearson's correlation coefficient
# is a standardized coefficient; the difference in the scale of the two
# correlated variables would be removed.
ratings.sub$size.z =
(ratings.sub$meanSizeRating - sizeRate.mean) / sizeRate.sd
ratings.sub$weight.z =
(ratings.sub$meanWeightRating - weightRate.mean) / weightRate.sd
# Calculate the sum of products by multiplying the two z-scored variables.
sizeWeight.sp = sum(ratings.sub$size.z * ratings.sub$weight.z)
# Divide the sum of products by the degree of freedom (i.e., the number of
# paired observations minus 1), we get Pearson's r.
sizeWeight.r = sizeWeight.sp / (nrow(ratings.sub) - 1) # r = 0.999
# Run a one-sample t-test assuming 0 as the population mean.
# Get the t-value with Pearson's r, r-squared, the number paired observations,
# and the degree of freedom; check formula (9) in the Week 7-9 Handout.
sizeWeight.t = sizeWeight.r /
sqrt((1 - sizeWeight.r ^ 2) / (nrow(ratings.sub) - 2)) # t = 241.14
# Get a one-tailed p-value from a t-distribution using pt(). Since t-value
# is positive, get the upper-tail p-value by setting lower.tail to FALSE.
sizeWeight.p = pt(q = sizeWeight.t, df = nrow(ratings.sub) - 2,
lower.tail = FALSE)
# Get a two-tailed p, which is smaller than .001, so our Pearson's r is
# significantly different from 0.
sizeWeight.p * 2 # p < .001
# Verify our statistics using cor.test()
cor.test(ratings.sub$meanWeightRating, ratings.sub$meanSizeRating)
# Same r, t, and p (< .001).
# We tested the correlation between mean size rating and mean weight rating in
# Pearson's correlation test, and the result suggests a significant positive
# correlation (r = .999, t(78) = 241.14, p < .001). The effect size is also
# very large (r2 = .999).
# Task III
# I probably do not have to explain too much regarding how to using the base
# plot() function to generate a scatter plot...? The only thing that is probably
# worth mentioning is that it does matter which vairable is presented on the
# x-axis or y-axis. We are not talking about which variable is explained by
# another variable, but just how two variables covary with each other.
plot(ratings.sub$size.z, ratings.sub$weight.z,
main = "Correlation between mean size and\n mean weight ratings of English nouns",
xlab = "Size Rating (z-score)", ylab = "Weight Rating (z-score)")
# Bonus
library(ggplot2)
ggplot(mapping = aes(x = size.z, y = weight.z), data = ratings.sub) +
geom_point(alpha = 0.5, size = 3, color = "darkgrey") +
# This is the thing needed for adding the trend line generated with simple
# linear regression with the two variables. But since it is part of
# Assignment X, it's OK if you have not mastered this part just yet.
geom_smooth(method = "lm", lwd = 1.5, color = "red") +
labs(title = "Correlation between mean size and mean weight ratings of English nouns,",
x = "Size Rating (z-score)", y = "Weight Rating (z-score)",
caption = "r = 0.999, t(78) = 241.14, p < .001") + theme_bw()
ggplot(mapping = aes(x = size.z, y = weight.z), data = ratings.sub) +
geom_point(alpha = 0.5, size = 3, color = "darkgrey") +
# This is the thing needed for adding the trend line generated with simple
# linear regression with the two variables. But since it is part of
# Assignment X, it's OK if you have not mastered this part just yet.
geom_smooth(method = "lm", lwd = 1.5, color = "red") +
labs(title = "Correlation between mean size and \nmean weight ratings of English nouns",
x = "Size Rating (z-score)", y = "Weight Rating (z-score)",
caption = "r = 0.999, t(78) = 241.14, p < .001") + theme_bw()
# Part II
# Task IV
# Get the two samples of mean size ratings based on the two classes first.
plant.size = ratings.sub[ratings.sub$Class == "plant",]$meanSizeRating
animal.size = ratings.sub[ratings.sub$Class == "animal",]$meanSizeRating
# Use var() to get the variance of the two samples.
plant.var = var(plant.size) # 0.087
animal.var = var(animal.size) # 0.181
# Calculate the F-value manually by dividing one variance by another.
# In my case, I divide the variance of the "plant" sample by that of the
# "animal" sample, and get a variance ratio of 0.479.
plant.animal.f = plant.var / animal.var # 0.479
# Get a one-tailed p-value with an F-value from an F-distribution based on two
# dfs, which are the size of the two-samples minus 1. Since my F-value is
# smaller than 1, which is the assumed population mean, my one-tailed p-value
# is from the lower tail of the F-distribution, so I don't have to set
# "lower.tail" to TRUE because by default it is true.
plant.animal.p = pf(q = plant.animal.f, df1 = length(plant.size) - 1,
df2 = length(animal.size) - 1)
# Get a two-tailed p
plant.animal.p * 2 # p = .028 < .05
# According to the F-test manually done above, we have a variance ratio that
# is very unlikely to be observed due to chance/randomness, it is likely the
# two samples do not have an equal variance, so we need to run a two-sample
# unpaired t-test assuming an unequal variance.
# Task V
# A two-sample unpaired t-test assuming an unequal variance for comparing the
# difference in mean size ratings between the "plant" and "animal" samples.
t.test(plant.size, animal.size)
# We tested the difference in mean size ratings between "plant" nouns and
# "animal" nouns in a two-sample unpaired t-test assuming an unequal variance,
# which suggests a significant difference between the two sample means
# t(77.055) = -21.62, p < .001). The 95% CI suggests that if this study is
# repeated for another 99 times, this study will be one of the 95 studies with a
# CI that covers the true population mean. Since the 95% CI in the current study
# does not cover 0, it means that when we say the true population mean is not 0,
# there is only 5% of chance we will be wrong. Thus, the true difference in
# mean size ratings between the two groups is very unlikely to be 0. In sum,
# the hypothesis that "plant" nouns are generally rated to have a smaller size
# (mean = 1.903) than "animal" nouns (mean = 3.65) could be true.
# Bonus
# Since the effect of noun class is significant, we can calculate Cohen's d
# for the size of the effect, which is just the difference between the two
# sample means divided by the SD of all data points.
abs(mean(plant.size) - mean(animal.size)) / sd(ratings.sub$meanSizeRating)
# 1.84 is much higher than 0.8, the boundary separating medium effect sizes from
# large effect sizes. So, we get a very large effect size.
# Part III
# Task VI
# Two-way independent-measure ANOVA explaining mean size rating by noun class,
# morphological structure, and their two-way interaction.
ratings.aov = aov(meanSizeRating ~ Class * Complex, data = ratings.sub)
summary(ratings.aov)
# To test the effects of noun class and morphological structure on mean size
# ratings, we ran a two-way independent-measure ANOVA including mean size
# rating as the dependent variable and noun class and morphological structure
# as the independent variables. The two-way interaction between the two
# independent variables. We found a significant main effect of noun class
# (F(1, 76) = 420.38, p < .001), but there was no significant effect of
# morphological structure (F(1, 76) = 0.143, p = .707) or the two-way
# interaction (F(1, 76) = 0.558, p = . 457).
# Task VII
TukeyHSD(ratings.aov)
# The post-hoc pairwise comparisons found significant differences only between
# all comparisons between plant and animal nouns, which are aligned with the
# only main effect of noun class in the above ANOVA analysis. These findings
# go against the new hypotheses in Part III, because neither did we find a
# significant main effect of morphological structure, nor did the significant
# main effect of noun class vary by morphological structure. The only thing
# that matters to mean size ratings in our analysis so far is whether a noun
# represents a plant or an animal.
# Task VIII.a
# Just some old tricks with the base boxplot() function.
boxplot(meanSizeRating ~ Class,
main = "Distribution of Mean Size Rating by English Noun Class",
xlabs = "Noun Class", ylabs = "Mean Ratings", data = ratings.sub)
# Task VIII.b
library(ggplot2)
ggplot(mapping = aes(x = Class, y = meanSizeRating), data = ratings.sub) +
geom_boxplot() +
# This is how you divide plots into multiple columns by a variable with
# facet_grid()
facet_grid(~ Complex) +
labs(title = "Distribution of Mean Size Rating by English Noun Class",
x = "Noun Class", y = "Mean Size Rating") + theme_bw()
source("https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/courseUtil.R")
source("https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/courseUtil.R")
source("https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/main/courseUtil.R")
wordRT = loadCourseCSV(year = 2024, topic = "3_Data", file = "wordRT.csv")
head(wordRT)
resourcesURL = "https:// https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/main/2024/3_Data/"
# Use paste() to glue the URL and the file name together, and the "sep"
# parameter stands for the character separating every two strings, which is
# a string that represents a "Tab" key in the current example
wordRT =
read.csv(paste(resourcesURL, "wordRT.csv", sep = "\t"))
resourcesURL = "https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/main/2024/3_Data/"
# Use paste() to glue the URL and the file name together, and the "sep"
# parameter stands for the character separating every two strings, which is
# a string that represents a "Tab" key in the current example
wordRT =
read.csv(paste(resourcesURL, "wordRT.csv", sep = "\t"))
resourcesURL = "https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/main/2024/3_Data/"
wordRT =
read.csv(paste(resourcesURL, "wordRT.csv", sep = "\t"))
wordRT =
read.csv(paste(resourcesURL, "wordRT.csv", sep = ""))
head(wordRT)
jabberwocky.wd = loadCourseCSV(2004, "3_Data", "jabberwocky_words.txt")
jabberwocky.wd = loadCourseCSV(2024, "3_Data", "jabberwocky_words.txt")
jabberwocky.wordCat =
loadCourseCSV(2024, "3_Data", "jabberwocky_words_cat.csv")
head(jabberwocky.wordCat)
cat.na = is.na(jabberwocky.wordCat$Cat)
sum(cat.na)
setwd("C:/Users/shafe/OneDrive - NTHU/Academic Works/NTHU/Courses/Language and Statistics in R/GitHub/Statistics_in_R/2024/Assignments")
source("https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/main/courseUtil.R")
Myers.sample = loadCourseCSV(2024, "3_Data", "Myers_2015_Sample.csv")
head(Myers.sample)
Myers.sub = subset(Myers.sample, grepl("Z", ASCII))
head(Myers.sub)
Myers.sub = subset(Myers.sample, grepl("Z", ASCII) | grepl("p", ASCII)))
Myers.sub = subset(Myers.sample, grepl("Z", ASCII) | grepl("p", ASCII))
tail(Myers.sub)
setwd("C:/Users/shafe/OneDrive - NTHU/Academic Works/NTHU/Courses/Language and Statistics in R/GitHub/Statistics_in_R/2024/4_Parametric")
write.csv(Myers.sub, "MyersSub.csv", quote = F, row.names = F)
nrow(Myers.sub)
Myers.sub$logRT = log(Myers.sub$RT)
logRT.mean = mean(logRT.sd)
Myers.sub$logRT = log(Myers.sub$RT)
logRT.mean = mean(logRT.sd)
nrow(Myers.sub)
Myers.sub$logRT = log(Myers.sub$RT)
logRT.mean = mean(Myers.sub$logRT)
logRT.sd = sd(Myers.sub$logRT)
Myers.sub.clean = subset(Myers.sub, logRT < logRT.mean + 2.5 * logRT.sd |
logRT > logRT.mean - 2.5 * logRT.sd)
nrow(Myers.sub.clean)
nrow(Myers.sub)
Myers.sub.nozero = subset(Myers.sub, RT > 0)
Myers.sub.nozero$logRT = log(Myers.sub.nozero$RT)
logRT.mean = mean(Myers.sub.nozero$logRT)
logRT.sd = sd(Myers.sub.nozero$logRT)
Myers.sub.clean = subset(Myers.sub.nozero, logRT < logRT.mean + 2.5 * logRT.sd |
logRT > logRT.mean - 2.5 * logRT.sd)
nrow(Myers.sub.clean)
Myers.sub.avg = aggregate(Response ~ ItemID, FUN = mean, data = Myers.sub.clean)
Myers.sub.avg = aggregate(Response ~ ItemID, FUN = mean, data = Myers.sub.clean)
Myers.sub.avg = Myers.sub.avg[order(Myers.sub.avg$Response),]
head(Myers.sub.avg)
# Task 2
Myers.sub.avg = aggregate(Response ~ ItemID + ASCII + Item_ZhuyinFuhao,
FUN = mean, data = Myers.sub.clean)
Myers.sub.avg = Myers.sub.avg[order(Myers.sub.avg$Response),]
head(Myers.sub.avg)
tail(Myers.sub.avg)
tail(Myers.sub.avg, 20)
head(Myers.sub.avg, 20)
Myers.sub = subset(Myers.sample, grepl("Z", ASCII) | grepl("p", ASCII)))
Myers.sub = subset(Myers.sample, grepl("Z", ASCII) | grepl("p", ASCII))
head(Myers.sub)
Myers.sub$OnsetC = ifelse(grepl("^Z", Myers.sub$ASCII), "Z", "p")
head(Myers.sub)
write.csv(Myers.sub, "MyersSub.csv", quote = F, row.names = F)
# Task 1
Myers.sub.nozero = subset(Myers.sub, RT > 0)
Myers.sub.nozero$logRT = log(Myers.sub.nozero$RT)
logRT.mean = mean(Myers.sub.nozero$logRT)
logRT.sd = sd(Myers.sub.nozero$logRT)
Myers.sub.clean = subset(Myers.sub.nozero, logRT < logRT.mean + 2.5 * logRT.sd |
logRT > logRT.mean - 2.5 * logRT.sd)
nrow(Myers.sub.clean)
# Task 2
Myers.sub.avg = aggregate(Response ~ ItemID + ASCII + Item_ZhuyinFuhao + OnsetC,
FUN = mean, data = Myers.sub.clean)
Myers.sub.avg = Myers.sub.avg[order(Myers.sub.avg$Response),]
head(Myers.sub.avg)
tail(Myers.sub.avg)
tail(Myers.sub.avg, 20)
# Task 4
var(Myers.sub.avg[Myers.sub.avg$OnsetC == "Z",]$Response,
Myers.sub.avg[Myers.sub.avg$OnsetC == "p",]$Response)
# Task 4
var.test(Myers.sub.avg[Myers.sub.avg$OnsetC == "Z",]$Response,
Myers.sub.avg[Myers.sub.avg$OnsetC == "p",]$Response)
t.test(Response ~ OnsetC, data = Myers.sub.avg)
# Task 5
library(ggplot2)
ggplot(data = Myers.sub.avg, mapping = aes(x = OnsetC, y = Response)) +
geom_violine() + geom_boxplot(width = 0.2) +
labs(title = "Mean Acceptability Rates in Myers (2015)", xlab = "Onset Consonant",
ylab = "Mean Acceptability Rate",
caption = "Z = voiced retroflex fricative\np = voiceless aspirated bilabial stop") +
theme_bw()
# Task 5
library(ggplot2)
ggplot(data = Myers.sub.avg, mapping = aes(x = OnsetC, y = Response)) +
geom_violin() + geom_boxplot(width = 0.2) +
labs(title = "Mean Acceptability Rates in Myers (2015)", xlab = "Onset Consonant",
ylab = "Mean Acceptability Rate",
caption = "Z = voiced retroflex fricative\np = voiceless aspirated bilabial stop") +
theme_bw()
# Task 5
library(ggplot2)
ggplot(data = Myers.sub.avg, mapping = aes(x = OnsetC, y = Response)) +
geom_violin() + geom_boxplot(width = 0.1) +
labs(title = "Mean Acceptability Rates in Myers (2015)", xlab = "Onset Consonant",
ylab = "Mean Acceptability Rate",
caption = "Z = voiced retroflex fricative\np = voiceless aspirated bilabial stop") +
theme_bw()
# Task 5
library(ggplot2)
ggplot(data = Myers.sub.avg, mapping = aes(x = OnsetC, y = Response)) +
geom_violin() + geom_boxplot(width = 0.1) +
labs(title = "Mean Acceptability Rates in Myers (2015)", x = "Onset Consonant",
y = "Mean Acceptability Rate",
caption = "Z = voiced retroflex fricative\np = voiceless aspirated bilabial stop") +
theme_bw()
median(Myers.sub.avg[Myers.sub.avg$OnsetC == "Z",]$Response)
nrow(Myers.sub)
ggplot(data = Myers.sub.avg, mapping = aes(x = OnsetC, y = Response)) +
geom_violin() + geom_boxplot(width = 0.1) +
scale_y_continuous(limits = c(0, 1)) +
labs(title = "Mean Acceptability Rates in Myers (2015)", x = "Onset Consonant",
y = "Mean Acceptability Rate",
caption = "Z = voiced retroflex fricative\np = voiceless aspirated bilabial stop") +
theme_bw()
