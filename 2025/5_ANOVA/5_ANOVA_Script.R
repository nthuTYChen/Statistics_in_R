# Load the course script
source("https://raw.githubusercontent.com/nthuTYChen/Statistics_in_R/main/courseUtil.R")

# Load the Myers' (2015) clean data set to revisit the test of equal variance
# (aka F-test)
Myers.clean = loadCourseCSV(2024, "5_ANOVA", "MyersClean.csv")

# Extract the logRT of the two experiment sessions to test if the two samples
# have an equal variance
s1.rt = Myers.clean[Myers.clean$Session == 1,]$logRT
s2.rt = Myers.clean[Myers.clean$Session == 2,]$logRT

# Check the report of the F-test generated by var.test(); check the Unit 5
# handout for detailed explanations.
var.test(s1.rt, s2.rt)

# Get the variance (mean sum of squares) of each sample
var(s1.rt)
var(s2.rt)

# F-test formula: s2x / s2y
var(s1.rt) / var(s2.rt)

# DFs = each sample size - 1
length(s1.rt)
length(s2.rt)

# Get the p-value from an F distribution with two dfs derived from the
# numerator and denominator samples. The F value is higher than 1, which is the
# peak of an F distribution, so get an upper tail p-value first.
# Double the one-tailed p to get a two-tailed p, because the null hypothesis
# in an F-test is that the two variances are the same, which means that the 
# alternative hypothesis is that the numerator is higher/lower than the
# denominator.
pf(q = 1.160577, df1 = 6750, df2 = 7200, lower.tail = F) * 2

# Extend the F-test to a one-way independent-measures ANOVA
# Check the Unit 5 handout for the explanations of "one-way",
# "independent measures", and its null/alternative hypothesis.
# Check the Unit 5 handout for the detailed introduction to Saito & Lyster's
# (2012) study
sl.sim = loadCourseCSV(2025, "5_ANOVA", "SaitoLysterSim.csv")

# Use the y ~ x syntax (explain the dependent variable y with the independent
# variable/factor x) to build an ANOVA model and fit the model to the data
sl.sim.aov = aov(formula = rF3 ~ Group, data = sl.sim)
# Get the analysis summary
summary(sl.sim.aov)

# Validate the F-value: between-sample Mean Sq / within-sample Mean Sq (aka MSE)
5345500 / 107131

# Manually calculate SS, F, and p-value in one-way independent-measures ANOVA
# The global mean: The average value of the entire sample
mean.grand = mean(sl.sim$rF3)
# Calculate the three sample means
mean.control = mean(sl.sim[sl.sim$Group == "Control",]$rF3)
mean.ffi = mean(sl.sim[sl.sim$Group == "FFI",]$rF3)
mean.fficf = mean(sl.sim[sl.sim$Group == "FFI+CF",]$rF3)

# Get the three sample sizes
control.n = nrow(subset(sl.sim, Group == "Control"))
ffi.n = nrow(subset(sl.sim, Group == "FFI"))
fficf.n = nrow(subset(sl.sim, Group == "FFI+CF"))

# Calculate the between-samples sum of squares (SS-between): The squared differences
# between individual sample means and the global mean
ss.between = control.n * (mean.control - mean.grand) ^ 2 +
  ffi.n * (mean.ffi - mean.grand) ^ 2 +
  fficf.n * (mean.fficf - mean.grand) ^ 2
ss.between

# df-between is the number of levels in the only fixed factor (aka independent
# variable) minus 1
# Between-sample mean squared difference, or MS. The interesting variance.
ss.between / (3 - 1)

# Calculate the within-sample sum of squares (SS-within): The squared differences
# between individual data points in each sample and their sample mean.
ss.control = sum((sl.sim[sl.sim$Group == "Control",]$rF3 - mean.control) ^ 2)
ss.ffi = sum((sl.sim[sl.sim$Group == "FFI",]$rF3 - mean.ffi) ^ 2)
ss.fficf = sum((sl.sim[sl.sim$Group == "FFI+CF",]$rF3 - mean.fficf) ^ 2)
ss.within = sum(ss.control, ss.ffi, ss.fficf)
ss.within

# df-within is the sum of each sample size minus 1
ss.within / (control.n + ffi.n + fficf.n - 3)

# Get the two-tailed p-value using pf() based on the F-value calculated with
# MS / MSE.
pf(q = 49.9, df1 = 2, df2 = 637, lower.tail = F) * 2

# Leave out the FFI+CF so the Group factor only has two levels (FFI vs. Control)
# to demonstrate the similarity between an one-way independent-measures ANOVA
# and an unpaired two-sample t-test ASSUMING AN EQUAL VARIANCE. See the Unit 5
# handout for detailed explanations.
sl.sim.sub = subset(sl.sim, Group != "FFI+CF")
sl.sim.sub.aov = aov(rF3 ~ Group, data = sl.sim.sub)
summary(sl.sim.sub.aov)

# Run the unpaired t-test assuming an equal variance; the same df and p-value
# can be found.
t.test(formula = rF3 ~ Group, data = sl.sim.sub, var = TRUE)

# In this comparison, the squared t-value is equal to the F-value.
1.3268 ^ 2

# For pairwise comparisons, Bonferroni correction, and the TukeyHSD test, see 
# Unit 5 handout for detailed explanations.

# In the full ANOVA model, we can make three pairwise comparisons, so the default
# alpha level .05 should be divided by 3, which is equal to .0167
bonferroni.a = .05 / 3

# Make pairwise comparisons between every two levels in Group to test between-level
# differences. Only the FFI-Control comparison does not have a p-value lower than
# 0.167
t.test(formula = rF3 ~ Group, data = subset(sl.sim, Group != "FFI+CF"),
       var = TRUE)
t.test(formula = rF3 ~ Group, data = subset(sl.sim, Group != "Control"),
       var = TRUE)
t.test(formula = rF3 ~ Group, data = subset(sl.sim, Group != "FFI"),
       var = TRUE)

# Apply the TukeyHSD test to the entire ANOVA model for pairwise comparisons.
# Again, only the FFI-Control comparison does not have an adjusted p-value
# lower than .05
TukeyHSD(sl.sim.aov)

# One-way repeated-measures ANOVA
# See Unit 5 handout for the explanation of the within-subject design version
# of Saito and Lyster's simulated data
sl.rep.sim = loadCourseCSV(2025, "5_ANOVA", "SaitoLysterRepSim.csv")
str(sl.rep.sim)

# Calculate the by-subject rF3 average. WARNING: In real life, you SHOULD NOT
# do this because by-item variance also provides important information in
# statistical tests. I do this only for this demo. Please refer to Appendix B
# of the Unit 5 handout to see how to incorporate both by-subject and by-item
# analysis in ANOVA.
sl.rep.avg = aggregate(rF3 ~ Subject + Condition, FUN = mean, data = sl.rep.sim)

# Run the independent-measures ANOVA just for comparison; this is NOT the most
# appropriate ANOVA for a study with a within-subject design.
sl.rep.aov = aov(formula = rF3 ~ Condition, data = sl.rep.avg)
summary(sl.rep.aov)

# Run the MORE APPROPRIATE repeated-measures ANOVA
# First, convert Subject into a factor so the subject ID numbers are treated as 
# "unit labels" rather than numeric values.
sl.rep.avg$Subject = as.factor(sl.rep.avg$Subject)
# Build the ANOVA model; the Error() term means "Condition is a within-subject
# design factor, so partition out the between-subject variance across the three
# paired samples.
sl.rep.aov.rp = aov(rF3 ~ Condition + Error(Subject / Condition),
                    data = sl.rep.avg)
# Compare the two ANOVA outputs and see the similarities and the differences.
# See the Unit 5 handout for detailed explanations.
summary(sl.rep.aov.rp)

# Calculate the stats in one-way repeated-measures ANOVA manually to validate
# the numbers seen in sl.aov.rep

# The global/grand mean of rF3
mean.grand = mean(sl.rep.avg$rF3)

# The total variance (Sum of Squares): The squared differences between
# every single data point and the global mean
ss.total = sum((sl.rep.avg$rF3 - mean.grand) ^ 2)
ss.total

# In the one-way independent-measures ANOVA, SStotal = SSbetween + SSerror
26222 + 48823
# In the one-way repeated-measures ANOVA, 
# SStotal = SSbetween-subject + SSbetween + SSerror
18996 + 26222 + 29827

# Get the size of each paired sample
control.n = nrow(subset(sl.rep.avg, Condition == "Control"))
ffi.n = nrow(subset(sl.rep.avg, Condition == "FFI"))
fficf.n = nrow(subset(sl.rep.avg, Condition == "FFI+CF"))

# Get the mean rF3 of each paired sample
mean.control = mean(sl.rep.avg[sl.rep.avg$Condition == "Control",]$rF3)
mean.ffi = mean(sl.rep.avg[sl.rep.avg$Condition == "FFI",]$rF3)
mean.fficf = mean(sl.rep.avg[sl.rep.avg$Condition == "FFI+CF",]$rF3)

# Calculate the interesting variance (the between-sample SS); this part is the
# same as in a one-way independent-measures ANOVA
ss.between = control.n * (mean.control - mean.grand) ^ 2 +
  ffi.n * (mean.ffi - mean.grand) ^ 2 + fficf.n * (mean.fficf - mean.grand) ^ 2
ss.between

# Calculate the by-subject average rF3 for the calculation of SSbetween-subject
sl.subj.mean = aggregate(rF3 ~ Subject, FUN = mean, data = sl.rep.avg)
sl.subj.mean

# Calculate the between-unit (between-subject) variance; the number of levels
# corresponding to each sample in the independent variable multiplies the sum 
# of the squared differences between each by-subject mean and the global mean.
ss.subj = 3 * sum((sl.subj.mean$rF3 - mean.grand) ^ 2)
ss.subj

# Subtracting SSbetween-sample and SSbetween-subject from SStotal to get
# ss-error (SSE).
ss.total - ss.between - ss.subj
# In the one-way independent-measures ANOVA, SSbetween-subject is not partioned
# out from the SStotal
ss.total - ss.between

# TukeyHSD() doesn't work for repeated-measures ANOVA models, so it is not
# possible to run Tukey's HSD test for post-hoc pairwise comparison.
TukeyHSD(sl.rep.aov.rp)

# That's OK, we still have Bonferroni correction. There are three pairwise
# comparisons (CL vs. FFI, CL vs. FFI+CF, FFI vs. FFI+CF), so the 
# Bonferroni-corrected alpha is .05 / 3 = .016666666.....
# Run the paired t-tests yourself following the Unit 5 handout to see which
# difference is significant at the level of .05 / 3.
.05 / 3